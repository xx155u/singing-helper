import gradio as gr
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import soundfile as sf
import tempfile
import os
from librosa.sequence import dtw
import cv2
import threading
import time
from collections import deque
import subprocess
from librosa.onset import onset_detect, onset_strength

# ä¸­æ–‡å­—é«”è¨­å®š
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# --- å…¨åŸŸè¨­å®š (Global Settings) ---
TARGET_SR = 16000
HOP_LENGTH = 512


def get_audio_duration(audio_path):
    """ç²å–éŸ³è¨Šæª”æ¡ˆçš„æ™‚é•·ï¼ˆç§’ï¼‰ã€‚"""
    try:
        info = sf.info(audio_path)
        return info.duration
    except Exception as e:
        print(f"ç„¡æ³•ç²å–éŸ³è¨Šæ™‚é•·: {e}")
        return 30.0


def format_time(seconds):
    """å°‡ç§’æ•¸æ ¼å¼åŒ–ç‚ºæ˜“è®€çš„æ™‚é–“å­—ä¸²"""
    if seconds >= 60:
        return f"{int(seconds // 60)} åˆ† {int(seconds % 60)} ç§’"
    else:
        return f"{int(seconds)} ç§’"


def extract_pseudo_onsets(features, smooth=5, threshold_ratio=0.3):
    """ç”± RMS ç‰¹å¾µåµæ¸¬ pseudo-onsetsï¼ˆç¯€å¥å¼·æ‹ï¼‰ã€‚"""
    rms = features[:, -1]
    rms_smooth = np.convolve(rms, np.ones(smooth) / smooth, mode='same')
    rms_smooth = (rms_smooth - rms_smooth.min()) / (np.ptp(rms_smooth) + 1e-8)

    th = rms_smooth.mean() + threshold_ratio * rms_smooth.std()
    onsets = [i for i in range(1, len(rms_smooth))
              if rms_smooth[i] > th and rms_smooth[i] > rms_smooth[i-1]]
    return np.asarray(onsets, dtype=float)


def tempo_similarity(feat_in, feat_ref):
    """å›å‚³ 0â€“100 çš„ç¯€å¥åˆ†æ•¸ï¼Œè¶Šé«˜è¶ŠåŒæ­¥"""
    on1, on2 = extract_pseudo_onsets(feat_in), extract_pseudo_onsets(feat_ref)
    if len(on1) < 2 or len(on2) < 2:
        return 20.0

    L = max(len(on1), len(on2))
    a = np.pad(on1, (0, L-len(on1)), 'edge').reshape(-1, 1)
    b = np.pad(on2, (0, L-len(on2)), 'edge').reshape(-1, 1)

    D, _ = dtw(a, b)
    dist = D[-1, -1]
    score = 100 * np.exp(-dist / 50)
    return float(np.clip(score, 0, 100))


def generate_pitch_animation_video(pitch_timeline, mixed_audio_path, output_path=None):
    """ä½¿ç”¨ç´” matplotlib + ffmpeg ç”Ÿæˆå‹•ç•«å½±ç‰‡"""
    if not pitch_timeline:
        print("âš ï¸ ç„¡éŸ³é«˜æ™‚é–“è»¸æ•¸æ“šï¼Œè·³éå½±ç‰‡ç”Ÿæˆ")
        return None
    
    if output_path is None:
        output_path = tempfile.NamedTemporaryFile(suffix=".mp4", delete=False).name
    
    print("ğŸ“Š é–‹å§‹ç”Ÿæˆå‹•ç•«å½±ç‰‡...")
    
    times = np.array([d['time'] for d in pitch_timeline])
    pitch_diffs = np.array([d['pitch_diff'] for d in pitch_timeline])
    similarities = np.array([d['similarity'] for d in pitch_timeline])
    
    duration = times[-1]
    fps = 20
    total_frames = int(duration * fps)
    
    fig = plt.figure(figsize=(14, 10))
    
    def draw_frame(frame_idx):
        current_time = frame_idx / fps
        fig.clear()
        
        gs = fig.add_gridspec(3, 1, height_ratios=[1, 2, 2], hspace=0.3)
        
        ax_status = fig.add_subplot(gs[0])
        ax_status.axis('off')
        
        if len(times) > 0:
            idx = np.argmin(np.abs(times - current_time))
            current_data = pitch_timeline[idx]
            
            pitch_diff = current_data['pitch_diff']
            similarity = current_data['similarity']
            
            if abs(pitch_diff) < 0.5:
                status = "[æº–ç¢º] éŸ³æº–æº–ç¢º"
                status_color = 'green'
            elif abs(pitch_diff) < 1.5:
                status = "[æ³¨æ„] è¼•å¾®åå·®"
                status_color = 'orange'
            else:
                status = "[è­¦å‘Š] æ˜é¡¯åå·®"
                status_color = 'red'
            
            direction = "åé«˜" if pitch_diff > 0 else "åä½" if pitch_diff < 0 else "æº–ç¢º"
            
            bbox_props = dict(boxstyle='round,pad=0.5', facecolor=status_color, alpha=0.2)
            status_text = f"{status}\nç•¶å‰æ™‚é–“: {current_time:.2f} ç§’"
            ax_status.text(0.5, 0.7, status_text, 
                          fontsize=18, fontweight='bold', 
                          ha='center', va='center', 
                          color=status_color, bbox=bbox_props)
            
            detail_text = (f"éŸ³é«˜åå·®: {abs(pitch_diff):.2f} åŠéŸ³ ({direction}) | "
                          f"ç›¸ä¼¼åº¦: {similarity:.1f} åˆ†")
            ax_status.text(0.5, 0.3, detail_text,
                          fontsize=11, ha='center', va='center', color='black')
        
        ax_pitch = fig.add_subplot(gs[1])
        window_size = 3.0
        x_min = max(0, current_time - window_size / 2)
        x_max = current_time + window_size / 2
        
        mask_played = times <= current_time
        mask_future = times > current_time
        
        colors_played = ['red' if abs(pd) > 1.5 else 'orange' if abs(pd) > 0.5 else 'green' 
                         for pd in pitch_diffs[mask_played]]
        
        if np.any(mask_played):
            ax_pitch.scatter(times[mask_played], pitch_diffs[mask_played], 
                            c=colors_played, alpha=0.8, s=50, zorder=3, edgecolors='black', linewidth=0.5)
            ax_pitch.plot(times[mask_played], pitch_diffs[mask_played], 
                         color='gray', alpha=0.5, linewidth=2, zorder=2)
        
        if np.any(mask_future):
            ax_pitch.scatter(times[mask_future], pitch_diffs[mask_future], 
                            c='lightgray', alpha=0.3, s=30, zorder=1)
        
        ax_pitch.axvline(x=current_time, color='blue', linestyle='-', linewidth=3, zorder=4)
        ax_pitch.axhspan(-0.5, 0.5, alpha=0.15, color='green', zorder=0)
        ax_pitch.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)
        
        ax_pitch.set_xlabel('æ™‚é–“ (ç§’)', fontsize=11)
        ax_pitch.set_ylabel('éŸ³é«˜åå·® (åŠéŸ³)', fontsize=11)
        ax_pitch.set_title('å³æ™‚éŸ³é«˜åå·®è¿½è¹¤', fontsize=13, fontweight='bold')
        ax_pitch.grid(True, alpha=0.3)
        ax_pitch.set_ylim(-6, 6)
        ax_pitch.set_xlim(x_min, x_max)
        
        ax_similarity = fig.add_subplot(gs[2])
        
        if np.any(mask_played):
            ax_similarity.plot(times[mask_played], similarities[mask_played], 
                              color='#2E86AB', linewidth=3, zorder=3)
            ax_similarity.fill_between(times[mask_played], similarities[mask_played], 0,
                                      alpha=0.3, color='#2E86AB', zorder=2)
        
        if np.any(mask_future):
            ax_similarity.plot(times[mask_future], similarities[mask_future], 
                              color='lightgray', linewidth=2, alpha=0.5, zorder=1)
        
        ax_similarity.axvline(x=current_time, color='blue', linestyle='-', linewidth=3, zorder=4)
        ax_similarity.axhline(y=70, color='gray', linestyle='--', alpha=0.5)
        
        ax_similarity.set_xlabel('æ™‚é–“ (ç§’)', fontsize=11)
        ax_similarity.set_ylabel('ç›¸ä¼¼åº¦åˆ†æ•¸', fontsize=11)
        ax_similarity.set_title('å³æ™‚éŸ³æº–ç›¸ä¼¼åº¦', fontsize=13, fontweight='bold')
        ax_similarity.grid(True, alpha=0.3)
        ax_similarity.set_ylim(0, 105)
        ax_similarity.set_xlim(x_min, x_max)

    temp_video = output_path.replace('.mp4', '_no_audio.mp4')
    
    try:
        print("â³ æ­£åœ¨æ¸²æŸ“å‹•ç•«å¹€...")
        writer = animation.FFMpegWriter(fps=fps, bitrate=1800, codec='libx264')
        
        with writer.saving(fig, temp_video, dpi=100):
            for i in range(total_frames):
                draw_frame(i)
                writer.grab_frame()

        plt.close(fig)
        
        print("ğŸµ æ­£åœ¨åˆä½µéŸ³è¨Š...")

        result = subprocess.run([
            'ffmpeg', '-y', '-loglevel', 'error',
            '-i', temp_video,
            '-i', mixed_audio_path,
            '-c:v', 'copy',
            '-c:a', 'aac',
            '-shortest',
            output_path
        ], capture_output=True, text=True)
        
        if result.returncode != 0:
            print(f"âš ï¸ ffmpeg è­¦å‘Š: {result.stderr}")
        
        if os.path.exists(temp_video):
            os.remove(temp_video)
        
        print(f"âœ… å‹•ç•«å½±ç‰‡ç”Ÿæˆå®Œæˆï¼")
        return output_path
        
    except FileNotFoundError:
        print("âŒ éŒ¯èª¤ï¼šç³»çµ±æœªå®‰è£ ffmpeg")
        return None
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå½±ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        if os.path.exists(temp_video):
            return temp_video
        return None


class PlaybackState:
    """æ’­æ”¾ç‹€æ…‹ç®¡ç†é¡"""
    def __init__(self):
        self.is_playing = False
        self.current_time = 0.0
        self.total_duration = 0.0
        self.pitch_data = []
        self.lock = threading.Lock()
        
playback_state = PlaybackState()


def precompute_pitch_differences(features_input, features_ref, interval=0.1):
    """é å…ˆè¨ˆç®—æ•´æ®µéŸ³è¨Šæ¯å€‹æ™‚é–“é»çš„éŸ³é«˜å·®ç•°ã€‚"""
    frames_per_interval = int(interval * TARGET_SR / HOP_LENGTH)
    min_frames = min(features_input.shape[0], features_ref.shape[0])
    pitch_timeline = []
    
    for frame_idx in range(0, min_frames, frames_per_interval):
        window_size = int(0.5 * TARGET_SR / HOP_LENGTH)
        end_idx = min(frame_idx + window_size, min_frames)
        
        if end_idx - frame_idx < 10:
            continue
        
        chroma_input = features_input[frame_idx:end_idx, :12]
        chroma_ref = features_ref[frame_idx:end_idx, :12]
        
        input_pitch = np.argmax(np.mean(chroma_input, axis=0))
        ref_pitch = np.argmax(np.mean(chroma_ref, axis=0))
        
        pitch_diff = (input_pitch - ref_pitch) % 12
        if pitch_diff > 6:
            pitch_diff = pitch_diff - 12
        
        chroma_input_norm = chroma_input / (np.linalg.norm(chroma_input, axis=1, keepdims=True) + 1e-8)
        chroma_ref_norm = chroma_ref / (np.linalg.norm(chroma_ref, axis=1, keepdims=True) + 1e-8)
        similarity = np.mean(np.sum(chroma_input_norm * chroma_ref_norm, axis=1)) * 100
        
        timestamp = frame_idx * HOP_LENGTH / TARGET_SR
        
        pitch_timeline.append({
            'time': timestamp,
            'pitch_diff': pitch_diff,
            'similarity': similarity,
            'input_pitch': input_pitch,
            'ref_pitch': ref_pitch
        })
    
    return pitch_timeline


def get_note_name(pitch_class):
    """å°‡éŸ³é«˜é¡åˆ¥è½‰æ›ç‚ºéŸ³ç¬¦åç¨±"""
    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    return note_names[int(pitch_class) % 12]


def windowed_pitch_analysis(features_input, features_ref, window_size=3.0, overlap=1.0):
    """ä½¿ç”¨æ»‘å‹•è¦–çª—åˆ†æéŸ³æº–ï¼Œæ¯å€‹è¦–çª—ç¨ç«‹é€²è¡Œ DTW å’ŒéŸ³æº–è©•ä¼°ã€‚"""
    frames_per_window = int(window_size * TARGET_SR / HOP_LENGTH)
    frames_per_step = int((window_size - overlap) * TARGET_SR / HOP_LENGTH)
    
    results = []
    min_frames = min(features_input.shape[0], features_ref.shape[0])
    
    window_start = 0
    window_idx = 0
    
    while window_start + frames_per_window <= min_frames:
        window_end = window_start + frames_per_window
        
        input_window = features_input[window_start:window_end]
        ref_window = features_ref[window_start:window_end]
        
        if input_window.shape[0] < 10 or ref_window.shape[0] < 10:
            window_start += frames_per_step
            continue
        
        try:
            D, wp = align_dtw(input_window, ref_window)
            
            chroma_input = input_window[:, :12]
            chroma_ref = ref_window[:, :12]
            
            pitch_score, pitch_deviation, pitch_direction = calculate_window_pitch_score(
                chroma_input, chroma_ref, D, wp
            )
            
            tempo_score = calculate_tempo_score(input_window, ref_window)
            overall_score = 0.7 * pitch_score + 0.3 * tempo_score
            
            time_start = window_start * HOP_LENGTH / TARGET_SR
            time_end = window_end * HOP_LENGTH / TARGET_SR
            
            results.append({
                'window_idx': window_idx,
                'time_start': time_start,
                'time_end': time_end,
                'time_center': (time_start + time_end) / 2,
                'pitch_score': pitch_score,
                'pitch_deviation': pitch_deviation,
                'pitch_direction': pitch_direction,
                'tempo_score': tempo_score,
                'overall_score': overall_score,
                'normalized_dtw_cost': D[-1, -1] / len(wp)
            })
            
        except Exception as e:
            print(f"è¦–çª— {window_idx} åˆ†æå¤±æ•—: {e}")
        
        window_start += frames_per_step
        window_idx += 1
    
    return results


def calculate_window_pitch_score(chroma_input, chroma_ref, D, wp):
    """è¨ˆç®—è¦–çª—å…§çš„éŸ³æº–åˆ†æ•¸å’Œåå·®ã€‚"""
    input_peak_bins = np.argmax(chroma_input, axis=1)
    ref_peak_bins = np.argmax(chroma_ref, axis=1)
    
    pitch_diff = np.mean(input_peak_bins) - np.mean(ref_peak_bins)
    
    chroma_input_norm = chroma_input / (np.linalg.norm(chroma_input, axis=1, keepdims=True) + 1e-8)
    chroma_ref_norm = chroma_ref / (np.linalg.norm(chroma_ref, axis=1, keepdims=True) + 1e-8)
    
    cosine_similarities = np.sum(chroma_input_norm * chroma_ref_norm, axis=1)
    avg_similarity = np.mean(cosine_similarities)
    
    pitch_score = max(0, min(100, avg_similarity * 100))
    
    if abs(pitch_diff) < 0.5:
        pitch_direction = "æº–ç¢º"
    elif pitch_diff > 0:
        pitch_direction = "åé«˜"
    else:
        pitch_direction = "åä½"
    
    return pitch_score, abs(pitch_diff), pitch_direction


def calculate_tempo_score(input_feat, ref_feat):
    """ç¯€å¥åˆ†æ•¸ (0â€“100)ã€‚"""
    return tempo_similarity(input_feat, ref_feat)


def plot_windowed_analysis(results):
    """ç¹ªè£½è¦–çª—åˆ†æçµæœçš„æ™‚é–“åºåˆ—åœ–ã€‚"""
    if not results:
        fig = plt.figure(figsize=(12, 6))
        plt.text(0.5, 0.5, 'ç„¡è¶³å¤ è³‡æ–™é€²è¡Œè¦–çª—åˆ†æ', 
                ha='center', va='center', fontsize=14)
        return fig
    
    times = [r['time_center'] for r in results]
    pitch_scores = [r['pitch_score'] for r in results]
    tempo_scores = [r['tempo_score'] for r in results]
    overall_scores = [r['overall_score'] for r in results]
    pitch_deviations = [r['pitch_deviation'] for r in results]
    
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    
    ax1 = axes[0]
    ax1.plot(times, pitch_scores, marker='o', label='éŸ³æº–åˆ†æ•¸', linewidth=2, color='#2E86AB')
    ax1.plot(times, tempo_scores, marker='s', label='ç¯€å¥åˆ†æ•¸', linewidth=2, color='#A23B72')
    ax1.plot(times, overall_scores, marker='^', label='æ•´é«”åˆ†æ•¸', linewidth=2.5, color='#F18F01')
    
    ax1.axhline(y=70, color='gray', linestyle='--', alpha=0.5, label='è‰¯å¥½é–€æª» (70åˆ†)')
    ax1.fill_between(times, 70, 100, alpha=0.1, color='green')
    ax1.fill_between(times, 0, 70, alpha=0.1, color='red')
    
    ax1.set_xlabel('æ™‚é–“ (ç§’)', fontsize=12)
    ax1.set_ylabel('åˆ†æ•¸ (0-100)', fontsize=12)
    ax1.set_title('å„æ™‚æ®µéŸ³æº–èˆ‡ç¯€å¥è©•ä¼°', fontsize=14, fontweight='bold')
    ax1.legend(loc='lower left', fontsize=10)
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 105)
    
    ax2 = axes[1]
    colors = ['red' if r['pitch_direction'] == 'åé«˜' else 'blue' if r['pitch_direction'] == 'åä½' else 'green' 
              for r in results]
    
    ax2.bar(times, pitch_deviations, width=2, color=colors, alpha=0.7, edgecolor='black')
    ax2.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='å¯æ¥å—åå·® (0.5åŠéŸ³)')
    
    ax2.set_xlabel('æ™‚é–“ (ç§’)', fontsize=12)
    ax2.set_ylabel('éŸ³é«˜åå·® (åŠéŸ³)', fontsize=12)
    ax2.set_title('å„æ™‚æ®µéŸ³é«˜åå·®åˆ†æ', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')
    
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='red', alpha=0.7, label='åé«˜'),
        Patch(facecolor='blue', alpha=0.7, label='åä½'),
        Patch(facecolor='green', alpha=0.7, label='æº–ç¢º')
    ]
    ax2.legend(handles=legend_elements, loc='upper left', fontsize=10)
    
    plt.tight_layout()
    return fig


def generate_windowed_feedback(results):
    """æ ¹æ“šè¦–çª—åˆ†æçµæœç”Ÿæˆæ–‡å­—å»ºè­°ã€‚"""
    if not results:
        return "éŸ³è¨Šé•·åº¦ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œè¦–çª—åˆ†æã€‚"
    
    feedback = []
    
    avg_pitch_score = np.mean([r['pitch_score'] for r in results])
    avg_tempo_score = np.mean([r['tempo_score'] for r in results])
    avg_overall = np.mean([r['overall_score'] for r in results])
    
    feedback.append(f"### ğŸ“Š æ•´é«”è¡¨ç¾çµ±è¨ˆ")
    feedback.append(f"- **å¹³å‡éŸ³æº–åˆ†æ•¸**: {avg_pitch_score:.1f} åˆ†")
    feedback.append(f"- **å¹³å‡ç¯€å¥åˆ†æ•¸**: {avg_tempo_score:.1f} åˆ†")
    feedback.append(f"- **å¹³å‡æ•´é«”åˆ†æ•¸**: {avg_overall:.1f} åˆ†")
    feedback.append("")
    
    problem_windows = [r for r in results if r['overall_score'] < 70]
    
    if problem_windows:
        feedback.append(f"### âš ï¸ éœ€è¦æ”¹é€²çš„æ™‚æ®µï¼ˆå…± {len(problem_windows)} å€‹ï¼‰")
        for i, window in enumerate(problem_windows[:5], 1):
            time_range = f"{window['time_start']:.1f}ç§’ - {window['time_end']:.1f}ç§’"
            
            issues = []
            if window['pitch_score'] < 70:
                issues.append(f"éŸ³æº–{window['pitch_direction']}ï¼ˆåå·® {window['pitch_deviation']:.1f} åŠéŸ³ï¼‰")
            if window['tempo_score'] < 70:
                issues.append("ç¯€å¥ä¸ç©©")
            
            feedback.append(f"{i}. **{time_range}**: {', '.join(issues)}")
        
        if len(problem_windows) > 5:
            feedback.append(f"   ... é‚„æœ‰ {len(problem_windows) - 5} å€‹æ™‚æ®µéœ€è¦æ³¨æ„")
    else:
        feedback.append("### âœ… è¡¨ç¾å„ªç•°")
        feedback.append("æ‰€æœ‰æ™‚æ®µçš„è¡¨ç¾éƒ½åœ¨è‰¯å¥½æ°´æº–ä»¥ä¸Šï¼Œè«‹ç¹¼çºŒä¿æŒï¼")
    
    feedback.append("")
    
    best_window = max(results, key=lambda r: r['overall_score'])
    feedback.append(f"### ğŸŒŸ æœ€ä½³æ™‚æ®µ")
    feedback.append(f"**{best_window['time_start']:.1f}ç§’ - {best_window['time_end']:.1f}ç§’** "
                   f"(æ•´é«”åˆ†æ•¸: {best_window['overall_score']:.1f})")
    
    return '\n'.join(feedback)


def extract_features(audio_path):
    """è¼‰å…¥éŸ³è¨Šã€æ¨™æº–åŒ–è™•ç†ï¼Œä¸¦æå– Chroma å’Œ RMS ç‰¹å¾µã€‚"""
    if not audio_path or not os.path.exists(audio_path):
        raise gr.Error("è«‹éŒ„è£½æˆ–ä¸Šå‚³æœ‰æ•ˆçš„éŸ³è¨Šæª”æ¡ˆã€‚")

    try:
        y, sr = librosa.load(audio_path, sr=TARGET_SR, mono=True)
    except Exception as e:
        raise gr.Error(f"è¼‰å…¥éŸ³è¨Šæª”æ¡ˆå¤±æ•—: {e}")

    MIN_SAMPLES = 2048
    if len(y) < MIN_SAMPLES:
        raise gr.Error(f"éŸ³è¨Šé•·åº¦éçŸ­ ({len(y)/sr:.2f} ç§’)ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆåˆ†æã€‚")

    chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=HOP_LENGTH)
    rms = librosa.feature.rms(y=y, hop_length=HOP_LENGTH)
    features = np.vstack([chroma, rms])
    
    return features.T


def align_dtw(features_input, features_ref):
    """åŸ·è¡Œå‹•æ…‹æ™‚é–“è¦æ•´ (DTW) ä»¥å°é½Šå…©æ®µéŸ³è¨Šçš„æ™‚é–“è»¸ã€‚"""
    D, wp = librosa.sequence.dtw(
        X=features_input.T,
        Y=features_ref.T,
        metric='euclidean'
    )
    return D, wp


def mix_audio(path1, path2):
    """å°‡å…©æ®µéŸ³è¨Šæ··åˆæˆä¸€å€‹æª”æ¡ˆä»¥ä¾¿æ–¼æ¯”è¼ƒã€‚"""
    try:
        y1, _ = librosa.load(path1, sr=TARGET_SR, mono=True)
        y2, _ = librosa.load(path2, sr=TARGET_SR, mono=True)

        len1, len2 = len(y1), len(y2)
        if len1 > len2:
            y2 = np.pad(y2, (0, len1 - len2))
        else:
            y1 = np.pad(y1, (0, len2 - len1))

        mixed = y1 + y2
        
        max_amp = np.max(np.abs(mixed))
        if max_amp > 0:
            mixed /= max_amp
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as fp:
            sf.write(fp.name, mixed, TARGET_SR)
            return fp.name
    except Exception as e:
        print(f"æ··åˆéŸ³è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None


# ============================================================
# ã€ä¿®æ”¹ã€‘ä¸»è¦è©•ä¼°å‡½æ•¸ - ç§»é™¤é€²åº¦æ¢ï¼Œä¿ç•™è³‡è¨Šå¡ç‰‡
# ============================================================
def singing_evaluator(input_audio_path, ref_audio_path, generate_video=True):
    """
    Gradio ä»‹é¢çš„ä¸»è¦è™•ç†å‡½æ•¸ã€‚
    ä¸ä½¿ç”¨é€²åº¦æ¢ï¼Œç›´æ¥è™•ç†ä¸¦è¿”å›çµæœã€‚
    """
    if not input_audio_path or not ref_audio_path:
        raise gr.Error("è«‹åŒæ™‚ä¸Šå‚³æˆ–éŒ„è£½æ‚¨çš„æ­Œè²å’Œåƒè€ƒéŸ³è¨Šã€‚")

    try:
        # 1. ç‰¹å¾µæå–
        features_input = extract_features(input_audio_path)
        features_ref = extract_features(ref_audio_path)
        
        # 2. è¦–çª—å¼åˆ†æ
        window_results = windowed_pitch_analysis(features_input, features_ref, 
                                                 window_size=3.0, overlap=1.0)
        
        # 3. è¨ˆç®—æ•´é«”åˆ†æ•¸
        if window_results:
            avg_score = np.mean([r['overall_score'] for r in window_results])
            similarity_score = f"{avg_score:.1f}"
        else:
            similarity_score = "N/A"
        
        # 4. ç”Ÿæˆæ–‡å­—å»ºè­°
        feedback_text = generate_windowed_feedback(window_results)
        
        # 5. å¯è¦–åŒ–è¦–çª—åˆ†æçµæœ
        windowed_plot = plot_windowed_analysis(window_results)

        # 6. æ··åˆéŸ³è¨Š
        mixed_audio_path = mix_audio(input_audio_path, ref_audio_path)
        
        # 7. é è¨ˆç®—å³æ™‚éŸ³é«˜æ•¸æ“š
        pitch_timeline = precompute_pitch_differences(features_input, features_ref, interval=0.1)
        
        # 8. ç”Ÿæˆå‹•ç•«å½±ç‰‡ï¼ˆå¯é¸ï¼‰
        animation_video_path = None
        if generate_video and pitch_timeline:
            try:
                animation_video_path = generate_pitch_animation_video(
                    pitch_timeline, mixed_audio_path
                )
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå‹•ç•«å½±ç‰‡å¤±æ•—: {e}")
                animation_video_path = None

        return (similarity_score, feedback_text, windowed_plot, 
                input_audio_path, ref_audio_path, mixed_audio_path,
                animation_video_path)
        
    except gr.Error as e:
        raise e
    except Exception as e:
        error_message = f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}"
        print(error_message)
        import traceback
        traceback.print_exc()
        raise gr.Error("åˆ†æå¤±æ•—ï¼Œè«‹æª¢æŸ¥æ‚¨çš„éŸ³è¨Šæª”æ¡ˆæ˜¯å¦æœ‰æ•ˆï¼Œæˆ–ç¨å¾Œå†è©¦ã€‚")


# ============================================================
# ã€æ–°å¢ã€‘ç”Ÿæˆé ä¼°æ™‚é–“è³‡è¨Šå¡ç‰‡ HTML
# ============================================================
def generate_info_card_html(input_audio_path, ref_audio_path, generate_video):
    """ç”Ÿæˆé¡¯ç¤ºé ä¼°æ™‚é–“çš„è³‡è¨Šå¡ç‰‡ HTML"""
    input_duration = get_audio_duration(input_audio_path)
    ref_duration = get_audio_duration(ref_audio_path)
    max_duration = max(input_duration, ref_duration)
    
    # é ä¼°è™•ç†æ™‚é–“
    if generate_video:
        estimated_time = max_duration * 2.5
    else:
        estimated_time = max_duration * 1.5
    estimated_time = max(estimated_time, 10.0)
    
    est_str = format_time(estimated_time)
    duration_str = format_time(max_duration)
    
    html = f"""
<div style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 12px; color: white; font-family: 'Microsoft YaHei', sans-serif; margin-bottom: 15px;">
    <div style="text-align: center; margin-bottom: 15px;">
        <span style="font-size: 20px; font-weight: bold;">â³ æ­£åœ¨åˆ†æä¸­...</span>
    </div>
    
    <div style="display: flex; justify-content: space-around; text-align: center;">
        <div>
            <div style="font-size: 14px; opacity: 0.9;">ğŸ“Š éŸ³è¨Šé•·åº¦</div>
            <div style="font-size: 18px; font-weight: bold; margin-top: 5px;">{duration_str}</div>
        </div>
        <div>
            <div style="font-size: 14px; opacity: 0.9;">â±ï¸ é ä¼°è™•ç†æ™‚é–“</div>
            <div style="font-size: 18px; font-weight: bold; margin-top: 5px;">{est_str}</div>
        </div>
    </div>
    
    <div style="margin-top: 15px; text-align: center; font-size: 13px; opacity: 0.8;">
        {"ğŸ¬ åŒ…å«å‹•ç•«å½±ç‰‡ç”Ÿæˆ" if generate_video else "ğŸ“Š åƒ…é€²è¡ŒéŸ³è¨Šåˆ†æ"}
    </div>
</div>
"""
    return html


def generate_complete_card_html(elapsed_time):
    """ç”Ÿæˆå®Œæˆç‹€æ…‹çš„è³‡è¨Šå¡ç‰‡ HTML"""
    elapsed_str = format_time(elapsed_time)
    
    html = f"""
<div style="padding: 20px; background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%); border-radius: 12px; color: white; font-family: 'Microsoft YaHei', sans-serif; margin-bottom: 15px;">
    <div style="text-align: center;">
        <span style="font-size: 24px;">âœ… åˆ†æå®Œæˆï¼</span>
    </div>
    <div style="margin-top: 10px; text-align: center; font-size: 14px;">
        ç¸½è€—æ™‚: {elapsed_str}
    </div>
</div>
"""
    return html


# --- Gradio ç•Œé¢å®šç¾© ---
title = "ğŸ™ï¸ AI æ­Œè²ç›¸ä¼¼æ€§è©•ä¼°èˆ‡è¼”åŠ©ç³»çµ± ğŸ¶"
description = (
    "ä¸Šå‚³æˆ–**å³æ™‚éŒ„è£½**æ‚¨çš„æ­Œè²å’Œåƒè€ƒéŸ³è¨Šï¼Œç³»çµ±å°‡ä½¿ç”¨**æ»‘å‹•è¦–çª—åˆ†æ**ï¼ˆæ¯3ç§’ä¸€å€‹è¦–çª—ï¼Œé‡ç–Š1ç§’ï¼‰ï¼Œ"
    "é€éå‹•æ…‹æ™‚é–“è¦æ•´ (DTW) æŠ€è¡“ï¼Œå°æ¯å€‹æ™‚æ®µçš„**éŸ³æº–ã€ç¯€å¥**é€²è¡Œç¨ç«‹è©•ä¼°ã€‚"
    "æä¾› **0-100 çš„æ•´é«”åˆ†æ•¸**ã€**æ™‚é–“åºåˆ—åˆ†æåœ–è¡¨**ï¼Œä»¥åŠ**å‹•æ…‹å½±ç‰‡å¼å³æ™‚éŸ³é«˜åˆ†æ**ã€‚"
)

with gr.Blocks(theme=gr.themes.Soft(), title=title) as demo:
    gr.Markdown(f"# {title}")
    gr.Markdown(description)

    with gr.Row():
        input_audio = gr.Audio(type="filepath", label="ğŸ¤ æ‚¨çš„æ­Œè² (Input)", sources=["upload", "microphone"])
        ref_audio = gr.Audio(type="filepath", label="ğŸ§ åƒè€ƒéŸ³è¨Š (Reference)", sources=["upload", "microphone"])
    
    with gr.Row():
        analyze_btn = gr.Button("ğŸš€ é–‹å§‹åˆ†æèˆ‡è©•ä¼°", variant="primary", scale=3)
        generate_video_checkbox = gr.Checkbox(label="ç”Ÿæˆå‹•ç•«å½±ç‰‡ï¼ˆéœ€è¦è¼ƒé•·æ™‚é–“ï¼‰", value=True, scale=1)
    
    # ã€æ–°å¢ã€‘è³‡è¨Šå¡ç‰‡é¡¯ç¤ºå€
    info_card_display = gr.HTML(value="", visible=False)
    
    result_outputs_group = gr.Column(visible=False) 
    with result_outputs_group:
        gr.Markdown("---")
        gr.Markdown("## ğŸ“‹ è©•ä¼°å ±å‘Š")
        
        with gr.Row():
            score_display = gr.Textbox(label="ç¸½é«”ç›¸ä¼¼åº¦åˆ†æ•¸ (0-100åˆ†ï¼Œè¶Šé«˜è¶Šå¥½)", scale=1)
        
        feedback_output = gr.Markdown(label="### ğŸ“œ å…·é«”æ”¹é€²å»ºè­°")

        windowed_plot_output = gr.Plot(label="ğŸ“Š è¦–çª—å¼åˆ†æåœ–è¡¨ (éŸ³æº–èˆ‡ç¯€å¥éš¨æ™‚é–“è®ŠåŒ–)")
        
        gr.Markdown("---")
        gr.Markdown("## ğŸ”Š éŸ³è¨Šæ¯”å°æ’­æ”¾")
        
        with gr.Row():
            input_playback = gr.Audio(label="æ‚¨çš„æ­Œè² (Input)")
            ref_playback = gr.Audio(label="åƒè€ƒéŸ³è¨Š (Reference)")
        
        mixed_playback = gr.Audio(label="ğŸ›ï¸ ç–ŠåŠ æ’­æ”¾ (Mixed for Comparison)")
        gr.Markdown("*æç¤ºï¼šæ’­æ”¾ã€Œç–ŠåŠ æ’­æ”¾ã€éŸ³è»Œï¼Œå¯ä»¥å¹«åŠ©æ‚¨æ›´æ¸…æ™°åœ°è½å‡ºç¯€å¥å’ŒéŸ³æº–çš„å·®ç•°ã€‚*")
        
        gr.Markdown("---")
        gr.Markdown("## ğŸ¬ å‹•æ…‹å³æ™‚éŸ³é«˜åˆ†æå½±ç‰‡")
        gr.Markdown("*å½±ç‰‡æœƒè‡ªå‹•èˆ‡éŸ³è¨ŠåŒæ­¥æ’­æ”¾ï¼Œå±•ç¤ºæ¯å€‹æ™‚åˆ»çš„éŸ³é«˜è®ŠåŒ–å’Œåå·®åˆ†æ*")
        
        animation_video_output = gr.Video(label="ğŸµ å³æ™‚éŸ³é«˜åˆ†æå‹•ç•«", autoplay=False)
        gr.Markdown("ğŸ’¡ **ä½¿ç”¨æç¤º**: é»æ“Šæ’­æ”¾æŒ‰éˆ•ï¼Œå½±ç‰‡æœƒåŒæ­¥é¡¯ç¤ºéŸ³é«˜åˆ†æå‹•ç•«ï¼Œè®“æ‚¨æ¸…æ¥šçœ‹åˆ°æ¯å€‹æ™‚é–“é»çš„è¡¨ç¾")

    # ä¸»åˆ†ææµç¨‹
    def run_analysis(input_audio_path, ref_audio_path, should_generate_video):
        start_time = time.time()
        
        (score, feedback, plot, _, _, mixed_path, 
        animation_path) = singing_evaluator(input_audio_path, ref_audio_path, 
                                            generate_video=should_generate_video)
        
        elapsed_time = time.time() - start_time
        complete_html = generate_complete_card_html(elapsed_time)
        
        return {
            info_card_display: gr.HTML(value=complete_html, visible=True),
            result_outputs_group: gr.Column(visible=True),
            score_display: score,
            feedback_output: feedback,
            windowed_plot_output: plot,
            input_playback: gr.Audio(value=input_audio_path, label="æ‚¨çš„æ­Œè² (Input)"),
            ref_playback: gr.Audio(value=ref_audio_path, label="åƒè€ƒéŸ³è¨Š (Reference)"),
            mixed_playback: gr.Audio(value=mixed_path, label="ğŸ›ï¸ ç–ŠåŠ æ’­æ”¾ (Mixed for Comparison)"),
            animation_video_output: gr.Video(value=animation_path, label="ğŸµ å³æ™‚éŸ³é«˜åˆ†æå‹•ç•«")
        }
    
    def prepare_ui(input_audio_path, ref_audio_path, should_generate_video):
        """
        éšæ®µ 1: é»æ“Šå¾Œç«‹å³åŸ·è¡Œ
        - é¡¯ç¤ºé ä¼°æ™‚é–“è³‡è¨Šå¡ç‰‡
        - éš±è—èˆŠçµæœ
        - é–å®šæŒ‰éˆ•
        """
        if not input_audio_path or not ref_audio_path:
            raise gr.Error("è«‹åŒæ™‚ä¸Šå‚³æˆ–éŒ„è£½æ‚¨çš„æ­Œè²å’Œåƒè€ƒéŸ³è¨Šã€‚")
        
        info_html = generate_info_card_html(input_audio_path, ref_audio_path, should_generate_video)
        
        return (
            gr.HTML(value=info_html, visible=True),
            gr.Column(visible=False),
            gr.Button(value="â³ è©•ä¼°ä¸­è«‹ç¨å¾Œ...", interactive=False)
        )

    def finish_analysis():
        """éšæ®µ 3: åˆ†æå®Œæˆå¾ŒåŸ·è¡Œ - æ¢å¾©æŒ‰éˆ•ç‹€æ…‹"""
        return gr.Button(value="ğŸš€ é–‹å§‹åˆ†æèˆ‡è©•ä¼°", interactive=True)
    
    # äº‹ä»¶ç¶å®š
    analyze_btn.click(
        fn=prepare_ui, 
        inputs=[input_audio, ref_audio, generate_video_checkbox], 
        outputs=[info_card_display, result_outputs_group, analyze_btn],
        queue=False
    ).then(
        fn=run_analysis,
        inputs=[input_audio, ref_audio, generate_video_checkbox],
        outputs=[info_card_display, result_outputs_group, score_display, feedback_output, windowed_plot_output, 
                 input_playback, ref_playback, mixed_playback, animation_video_output]
    ).then(
        fn=finish_analysis,
        inputs=[],
        outputs=[analyze_btn]
    )


if __name__ == "__main__":
    demo.launch(share=True)