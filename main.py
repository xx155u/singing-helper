# import gradio as gr
# import numpy as np
# import librosa
# import librosa.display
# import matplotlib.pyplot as plt
# import os
# import io

# # æ ¸å¿ƒé‚è¼¯ 1: ç‰¹å¾µæå– (Feature Extraction)
# def extract_features(audio_path, sr=22050):
#     """è¼‰å…¥éŸ³è¨Šä¸¦æå– Chroma (éŸ³é«˜/å’Œè²) å’Œ RMS (èƒ½é‡/éŸ³é‡) ç‰¹å¾µã€‚"""
#     try:
#         # è¼‰å…¥éŸ³è¨Š
#         y, sr = librosa.load(audio_path, sr=sr, mono=True)
#     except Exception as e:
#         # Gradio åœ¨éŒ„éŸ³æ™‚ï¼Œå¦‚æœä½¿ç”¨è€…æ²’æœ‰éŒ„è£½å…§å®¹å°±æäº¤ï¼Œå¯èƒ½æœƒå‚³å…¥ None æˆ–ç©ºè·¯å¾‘
#         if not audio_path:
#             raise gr.Error("è«‹éŒ„è£½æˆ–ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆã€‚")
#         raise gr.Error(f"è¼‰å…¥éŸ³è¨Šæª”æ¡ˆå¤±æ•—: {e}")

#     # --- æ–°å¢ ---
#     # æª¢æŸ¥éŸ³è¨Šé•·åº¦ï¼Œé¿å…å› æª”æ¡ˆéçŸ­å°è‡´ librosa åˆ†ææ™‚å‡ºéŒ¯
#     MIN_SAMPLES = 2048 # FFT é‹ç®—éœ€è¦ä¸€å®šçš„æ¨£æœ¬æ•¸
#     if len(y) < MIN_SAMPLES:
#         raise gr.Error(f"éŸ³è¨Šé•·åº¦éçŸ­ ({len(y)/sr:.2f} ç§’)ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆåˆ†æã€‚è«‹æä¾›è‡³å°‘ {MIN_SAMPLES/sr:.2f} ç§’çš„éŸ³è¨Šã€‚")

#     # 1. Chroma feature (éŸ³é«˜/å’Œè²å…§å®¹)
#     # ä½¿ç”¨ CQT (Constant-Q Transform) å¾—åˆ°æ›´å¥½çš„éŸ³é«˜è§£æåº¦ï¼Œå†è½‰æ›ç‚º Chroma
#     # Chroma å‘é‡ (12ç¶­) ä»£è¡¨äº†æ¯å€‹å…«åº¦å…§çš„éŸ³é«˜åˆ†ä½ˆ
#     chroma = librosa.feature.chroma_cqt(y=y, sr=sr)
    
#     # 2. RMS (èƒ½é‡/éŸ³é‡) - ä½œç‚ºç°¡å–®çš„æ­Œè²æ´»å‹•åµæ¸¬ (VAD)
#     rms = librosa.feature.rms(y=y)
    
#     # åˆä½µç‰¹å¾µï¼š[12ç¶­ Chroma, 1ç¶­ RMS]
#     # è½‰ç½®å¾Œæˆç‚º (N_frames, 13) çš„çŸ©é™£
#     features = np.vstack([chroma, rms])
    
#     # è¿”å›è½‰ç½®å¾Œçš„ç‰¹å¾µçŸ©é™£å’Œå–æ¨£ç‡
#     return features.T, sr, y

# # æ ¸å¿ƒé‚è¼¯ 2: DTW å°é½Š (DTW Alignment)
# def align_dtw(features_input, features_ref):
#     """
#     åŸ·è¡Œå‹•æ…‹æ™‚é–“è¦æ•´ (DTW) ä»¥å°é½Šå…©æ®µæ­Œè²çš„æ™‚é–“è»¸ã€‚
#     DTW å°‹æ‰¾å¾ (0, 0) åˆ° (N, M) æˆæœ¬æœ€ä½çš„è·¯å¾‘ï¼Œä»¥æœ€å°åŒ–æ™‚é–“è¦æ•´å¾Œçš„å·®ç•°ã€‚
#     """
#     # ä½¿ç”¨æ­å¹¾é‡Œå¾—è·é›¢ä½œç‚ºè·é›¢åº¦é‡ (metric)
#     D, wp = librosa.sequence.dtw(
#         X=features_input.T,  # X è»¸ (Input)
#         Y=features_ref.T,    # Y è»¸ (Reference)
#         metric='euclidean'
#     )

#     return D, wp

# # æ ¸å¿ƒé‚è¼¯ 3: çµæœåˆ†æèˆ‡å»ºè­°ç”Ÿæˆ (Analysis and Feedback Generation)
# def analyze_results(wp, D, features_input, features_ref, sr):
#     """æ ¹æ“š DTW çµæœåˆ†æé€Ÿåº¦å’ŒéŸ³é«˜å·®ç•°ï¼Œä¸¦ç”Ÿæˆå»ºè­°ã€‚"""
#     feedback = []
    
#     # è¨ˆç®—æ¯å¹€çš„æ™‚é–“é•·åº¦
#     HOP_LENGTH = 512 # Librosa é è¨­çš„ Frame Hop é•·åº¦
#     TIME_PER_FRAME = HOP_LENGTH / sr
    
#     # --- 1. æ•´é«”ç›¸ä¼¼åº¦åˆ†æ•¸ (Overall Similarity Score) ---
#     # ç¸½æˆæœ¬ (Total Cost) 
#     total_cost = D[-1, -1]

#     # TODO:  I need score normalized to 0~100
#     # æ­£è¦åŒ–æˆæœ¬ (Normalized Cost) ä½œç‚ºç›¸ä¼¼åº¦æŒ‡æ¨™ (å€¼è¶Šä½è¶Šç›¸ä¼¼)
#     normalized_cost = total_cost / len(wp)
#     similarity_score = f"{normalized_cost:.4f}"
    
#     # --- 2. æ•´é«”ç¯€å¥/é€Ÿåº¦åˆ†æ (Global Tempo Analysis) ---
#     input_frames = features_input.shape[0]
#     ref_frames = features_ref.shape[0]
#     avg_slope = input_frames / ref_frames
    
#     tempo_suggestion = "**æ•´é«”ç¯€å¥è©•ä¼° (Global Tempo):** "
#     if avg_slope > 1.15: # Input æ¯” Reference é•· 15% ä»¥ä¸Š (å”±å¾—å¤ªæ…¢)
#         tempo_suggestion += f"æ‚¨çš„æ­Œè²æ¯”åƒè€ƒéŸ³è¨Šæ…¢äº†ç´„ {avg_slope * 100 - 100:.1f}%ã€‚å»ºè­°æ‚¨æ•´é«”åŠ å¿«æ¼”å”±é€Ÿåº¦ã€‚"
#     elif avg_slope < 0.85: # Input æ¯” Reference çŸ­ 15% ä»¥ä¸Š (å”±å¾—å¤ªå¿«)
#         tempo_suggestion += f"æ‚¨çš„æ­Œè²æ¯”åƒè€ƒéŸ³è¨Šå¿«äº†ç´„ {100 - avg_slope * 100:.1f}%ã€‚å»ºè­°æ‚¨æ•´é«”æ”¾æ…¢æ¼”å”±é€Ÿåº¦ã€‚"
#     else:
#         tempo_suggestion += "æ‚¨çš„æ•´é«”ç¯€å¥æŒæ¡å¾—å¾ˆå¥½ï¼Œèˆ‡åƒè€ƒéŸ³è¨Šå¤§è‡´åŒæ­¥ã€‚"
    
#     feedback.append(tempo_suggestion)
    
#     # --- 3. å±€éƒ¨æ™‚åºèˆ‡éŸ³é«˜å•é¡Œ (Local Timing and Pitch Issues) ---
#     feedback.append("\n**å…·é«”å±€éƒ¨æ”¹é€²å»ºè­° (Local Feedback):**")
    
#     # è¨ˆç®—æ¯æ¢è·¯å¾‘é»çš„å±€éƒ¨æˆæœ¬ (Local Cost) ä½œç‚ºä¸åŒ¹é…ç¨‹åº¦çš„æŒ‡æ¨™
#     local_costs = np.zeros(len(wp))
#     for k, (i, j) in enumerate(wp):
#         # ä½¿ç”¨ cost matrix D çš„å€¼ï¼Œä¸¦é™¤ä»¥è·¯å¾‘è·é›¢ (i+j) ä¾†é€²è¡Œå±€éƒ¨æ¯”è¼ƒ
#         local_costs[k] = D[i, j] / (i + j + 1e-6) # åŠ ä¸Šä¸€å€‹æ¥µå°å€¼é¿å…é™¤ä»¥é›¶

#     # æ‰¾å‡ºå±€éƒ¨æˆæœ¬é¡¯è‘—é«˜æ–¼å¹³å‡å€¼ (ä¾‹å¦‚ 1.5 å€‹æ¨™æº–å·®ä»¥ä¸Š) çš„é»
#     mean_cost = np.mean(local_costs)
#     std_cost = np.std(local_costs)
#     threshold = mean_cost + 1.5 * std_cost 
    
#     high_cost_points = np.where(local_costs > threshold)[0]
    
#     if high_cost_points.size == 0:
#         feedback.append("â€¢ æ²’æœ‰æª¢æ¸¬åˆ°æ˜é¡¯çš„å±€éƒ¨æ™‚åºæˆ–éŸ³é«˜å•é¡Œï¼Œè¡¨ç¾å„ªç•°ï¼")
#     else:
#         # å°‡é€£çºŒçš„é«˜æˆæœ¬é»åˆ†çµ„ç‚ºå–®å€‹å•é¡Œå€æ®µ (Issue Segment)
#         issue_groups = []
#         if high_cost_points.size > 0:
#             current_group = [high_cost_points[0]]
#             for i in range(1, len(high_cost_points)):
#                 if high_cost_points[i] == high_cost_points[i-1] + 1:
#                     current_group.append(high_cost_points[i])
#                 else:
#                     issue_groups.append(current_group)
#                     current_group = [high_cost_points[i]]
#             issue_groups.append(current_group)
        
#         # é™åˆ¶æœ€å¤šé¡¯ç¤º 5 å€‹å»ºè­°ï¼Œé¿å…ç‰ˆé¢éé•·
#         for group in issue_groups:
#             start_k, end_k = group[0], group[-1]
            
#             # å°æ‡‰åˆ° Input å’Œ Reference çš„å¹€ç´¢å¼•
#             i_start, j_start = wp[start_k]
#             i_end, j_end = wp[end_k]
            
#             # å°‡ Input å¹€ç´¢å¼•è½‰æ›ç‚ºæ™‚é–“ (ç§’)
#             time_start = i_start * TIME_PER_FRAME
#             time_end = i_end * TIME_PER_FRAME
            
#             # --- å±€éƒ¨éŸ³é«˜ (Pitch) åˆ†æ ---
#             # Chroma ç‰¹å¾µä½æ–¼ features_input/ref çš„å‰ 12 è¡Œ (0-11)
#             chroma_input_seg = features_input[i_start:i_end+1, :12]
#             chroma_ref_seg = features_ref[j_start:j_end+1, :12]
            
#             pitch_suggestion = ""
#             if chroma_input_seg.size > 0 and chroma_ref_seg.size > 0:
#                 # è¨ˆç®—è©²å€æ®µå…§ Input å’Œ Ref çš„ä¸»è¦éŸ³é«˜ (Peak Chroma Bin)
#                 input_peak_bin = np.mean(np.argmax(chroma_input_seg, axis=1))
#                 ref_peak_bin = np.mean(np.argmax(chroma_ref_seg, axis=1))
#                 # pitch_diff æ­£å€¼: åé«˜ (Input Chroma Bin > Ref Chroma Bin)
#                 pitch_diff = input_peak_bin - ref_peak_bin 
                
#                 # è¨ˆç®—å±€éƒ¨é€Ÿåº¦ (Local Speed) å·®ç•°
#                 frames_input_seg = i_end - i_start
#                 frames_ref_seg = j_end - j_start
#                 local_slope = frames_input_seg / (frames_ref_seg + 1e-6) # é¿å…é™¤ä»¥é›¶

#                 if abs(pitch_diff) > 0.8: # éŸ³é«˜å·®ç•°è¶…éç´„ 0.8 å€‹åŠéŸ³
#                     pitch_level = "åé«˜" if pitch_diff > 0 else "åä½"
#                     pitch_suggestion = f"éŸ³é«˜æ˜é¡¯**{pitch_level}**ï¼Œå¹³å‡åå·®ç´„ {abs(pitch_diff):.1f} å€‹åŠéŸ³ã€‚"
#                 elif local_slope > 1.8:
#                     pitch_suggestion = "**æ™‚åºåš´é‡æ‹–æ²“**ï¼Œæ‚¨çš„æ¼”å”±å¤ªæ…¢äº†ï¼Œéœ€è¦æ›´æœæ–·åœ°é€²å…¥ä¸‹ä¸€å€‹æ¨‚å¥ã€‚"
#                 elif local_slope < 0.5:
#                     pitch_suggestion = "**æ™‚åºåš´é‡è¶…å‰**ï¼Œæ‚¨çš„æ¼”å”±å¤ªå¿«äº†ï¼Œè«‹ä»”ç´°è†è½åƒè€ƒéŸ³è¨Šçš„é–“éš”ã€‚"
#                 else:
#                     pitch_suggestion = "éŸ³æº–æˆ–éŸ³è‰²ä¸åŒ¹é…ã€‚è«‹å°ˆæ³¨æ–¼è©²æ¨‚å¥çš„éŸ³æº–ç©©å®šæ€§ã€‚"

#                 # TODO: why got 0 ç§’åˆ° 0 ç§’ï¼Ÿ
#                 feedback.append(f"â€¢ **æ™‚é–“ {time_start:.2f} ç§’åˆ° {time_end:.2f} ç§’:** {pitch_suggestion}")
        
#         if len(issue_groups) > 5:
#             feedback.append("â€¢ ... (åƒ…é¡¯ç¤ºå‰ 5 å€‹æœ€é¡¯è‘—çš„å•é¡Œé»)")

#     return similarity_score, '\n'.join(feedback)

# # æ ¸å¿ƒé‚è¼¯ 4: DTW å¯è¦–åŒ– (DTW Visualization)
# def plot_dtw_path(D, wp, sr):
#     """ç¹ªè£½ç´¯ç©æˆæœ¬çŸ©é™£å’Œæœ€ä½³è¦æ•´è·¯å¾‘ã€‚"""
#     fig = plt.figure(figsize=(10, 8))
#     ax = fig.add_subplot(111)
    
#     # é¡¯ç¤ºç´¯ç©æˆæœ¬çŸ©é™£ (Accumulated Cost Matrix)
#     # D çš„å½¢ç‹€ç‚º (input_frames, ref_frames)ã€‚specshow å°‡ç¬¬ä¸€ç¶­å°æ‡‰ y è»¸ï¼Œç¬¬äºŒç¶­å°æ‡‰ x è»¸ã€‚
#     img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='time', ax=ax)
#     fig.colorbar(img, ax=ax, format='%+2.0f', label='ç´¯ç©æˆæœ¬ (Accumulated Cost)')
#     ax.set(title='DTW ç´¯ç©æˆæœ¬çŸ©é™£èˆ‡æœ€ä½³è·¯å¾‘')
    
#     # ç¹ªè£½æœ€ä½³è¦æ•´è·¯å¾‘ (Warping Path)
#     # wp[:, 0] æ˜¯ input frames, wp[:, 1] æ˜¯ reference frames.
#     # æˆ‘å€‘ç¹ªè£½ (ref_time, input_time) ä¾†å°æ‡‰ x, y è»¸ã€‚
#     ax.plot(librosa.frames_to_time(wp[:, 1], sr=sr), librosa.frames_to_time(wp[:, 0], sr=sr), 
#             marker='o', color='red', linestyle='-', linewidth=2, alpha=0.5, 
#             label='æœ€ä½³è¦æ•´è·¯å¾‘ (Warping Path)')
    
#     ax.set_xlabel("åƒè€ƒéŸ³è¨Šæ™‚é–“ (Reference Time)")
#     ax.set_ylabel("æ‚¨çš„æ­Œè²æ™‚é–“ (Input Time)")
#     ax.legend(loc='lower right')
    
#     # --- ä¿®æ­£ ---
#     # Gradio çš„ gr.Plot å…ƒä»¶å¯ä»¥ç›´æ¥è™•ç† matplotlib çš„ Figure ç‰©ä»¶ï¼Œ
#     # é€™æ¯”æ‰‹å‹•è½‰æ›ç‚º bytes æ›´ç°¡æ½”ã€‚
#     # æˆ‘å€‘ä¸æ‡‰åœ¨æ­¤è™•å‘¼å« plt.close(fig)ï¼Œå¦å‰‡ Gradio å°‡ç„¡æ³•æ¸²æŸ“å®ƒã€‚
#     return fig


# # Gradio ä¸»è¦å‡½æ•¸
# def singing_evaluator(input_audio_path, ref_audio_path):
#     """Gradio æ¥å£çš„ä¸»è¦è™•ç†å‡½æ•¸ã€‚"""
#     if not input_audio_path or not ref_audio_path:
#         raise gr.Error("è«‹ä¸Šå‚³æˆ–éŒ„è£½æ‚¨çš„æ­Œè²å’Œåƒè€ƒéŸ³è¨Šæª”æ¡ˆã€‚")

#     try:
#         # 1. ç‰¹å¾µæå–
#         # TODO: make sure all to the same sr like 16,000 and mono, and if input m4a, mp3, etc., convert to flac first
#         features_input, sr, y_input = extract_features(input_audio_path)
#         features_ref, sr, y_ref = extract_features(ref_audio_path)
        
#         # 2. DTW å°é½Š
#         # TODO: use DTW is not strict enough? I input 2 different songs, it still give me a score, so also output confidence of the song,
#         # confidence lower than xx% means different songs, output "different songs" message
#         D, wp = align_dtw(features_input, features_ref)
        
#         # 3. åˆ†æä¸¦ç”Ÿæˆå»ºè­°
#         # TODO: fix issue in analyze_results
#         similarity_score, feedback_text = analyze_results(wp, D, features_input, features_ref, sr)
        
#         # 4. å¯è¦–åŒ– DTW è·¯å¾‘
#         # dtw_plot = plot_dtw_path(D, wp, sr)
        
#         # 5. è¿”å›çµæœ
#         return similarity_score, feedback_text, input_audio_path, ref_audio_path
        
#     except gr.Error as e:
#         # ç›´æ¥æ‹‹å‡º Gradio çš„éŒ¯èª¤ï¼Œä½¿å…¶èƒ½æ¸…æ™°åœ°é¡¯ç¤ºåœ¨ UI ä¸Š
#         raise e
#     except Exception as e:
#         error_message = f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}"
#         print(error_message) # åœ¨å¾Œç«¯æ‰“å°è©³ç´°éŒ¯èª¤ä»¥ä¾›èª¿è©¦
#         # å‘ç”¨æˆ¶é¡¯ç¤ºä¸€å€‹æ›´å‹å¥½çš„éŒ¯èª¤è¨Šæ¯
#         raise gr.Error("åˆ†æå¤±æ•—ï¼Œè«‹æª¢æŸ¥æ‚¨çš„éŸ³è¨Šæª”æ¡ˆæ˜¯å¦æœ‰æ•ˆï¼Œæˆ–ç¨å¾Œå†è©¦ã€‚")



# # --- Gradio ç•Œé¢å®šç¾© ---

# # æè¿°å’Œæ¨™é¡Œ
# title = "ğŸ™ï¸ AI æ­Œè²ç›¸ä¼¼æ€§è©•ä¼°èˆ‡è¼”åŠ©ç³»çµ±ğŸ¶"
# description = (
#     "ä¸Šå‚³æˆ–**å³æ™‚éŒ„è£½**å…©æ®µéŸ³è¨Šæª”æ¡ˆï¼ˆæ‚¨çš„æ­Œè²å’Œåƒè€ƒéŸ³è¨Šï¼‰ï¼Œç³»çµ±å°‡ä½¿ç”¨å‹•æ…‹æ™‚é–“è¦æ•´ (DTW) æŠ€è¡“å°é½ŠéŸ³è»Œï¼Œ"
#     "åˆ†æç¯€å¥å’ŒéŸ³é«˜å·®ç•°ï¼Œä¸¦æä¾›å…·é«”çš„æ”¹é€²å»ºè­°ï¼Œä¾‹å¦‚å¹¾ç§’åˆ°å¹¾ç§’å¤ªå¿«æˆ–éŸ³é«˜åä½ã€‚"
# )

# with gr.Blocks(theme=gr.themes.Soft(), title=title) as demo:
#     gr.Markdown(f"# {title}")
#     gr.Markdown(description)

#     # è¼¸å…¥å€ (å·²åŠ å…¥ microphone ä¾†æº)
#     with gr.Row():
#         input_audio_upload = gr.Audio(
#             type="filepath", 
#             label="ğŸ¤ æ‚¨çš„æ­Œè² (Input Audio)", 
#             sources=["upload", "microphone"] # æ”¯æ´ä¸Šå‚³å’ŒéŒ„éŸ³
#         )
#         ref_audio_upload = gr.Audio(
#             type="filepath", 
#             label="ğŸ§ åƒè€ƒéŸ³è¨Š (Reference Audio)", 
#             sources=["upload", "microphone"] # æ”¯æ´ä¸Šå‚³å’ŒéŒ„éŸ³
#         )
    
#     analyze_btn = gr.Button("ğŸš€ é–‹å§‹åˆ†æèˆ‡è©•ä¼°", variant="primary")
    
#     # è¼¸å‡ºå€
#     # å°‡ gr.Column è³¦å€¼çµ¦ä¸€å€‹è®Šæ•¸ï¼Œä»¥ä¾¿åœ¨ .success() ä¸­å¼•ç”¨
#     result_outputs_group = gr.Column(visible=False) 
#     with result_outputs_group:
#         gr.Markdown("---")
#         gr.Markdown("## ğŸ“‹ è©•ä¼°çµæœ")
        
#         with gr.Row():
#             score_display = gr.Textbox(label="ç¸½é«”æ­£è¦åŒ–ç›¸ä¼¼åº¦åˆ†æ•¸ (è¶Šä½è¶Šç›¸ä¼¼)", show_label=True, scale=1)
            
#         # å°‡ Markdown å…ƒä»¶ä¹Ÿè³¦å€¼çµ¦ä¸€å€‹è®Šæ•¸
#         feedback_output = gr.Markdown("### ğŸ“œ å…·é«”æ”¹é€²å»ºè­°")

#         dtw_plot_output = gr.Plot(label="DTW è¦æ•´è·¯å¾‘åœ– (Warping Path Visualization)")
        
#         gr.Markdown("---")
#         gr.Markdown("## ğŸ”Š åŒæ­¥æ’­æ”¾ (è«‹æ‰‹å‹•æŒ‰ä¸‹æ’­æ”¾éµ)")
        

#         # TODO: 
#         # 1. æ–°å¢åŒæ™‚æ’­æ”¾å…©å€‹åŸæœ¬éŸ³è¨ŠåŠŸèƒ½ï¼Œä¹Ÿå°±æ˜¯ç–ŠåŠ 
#         # 2. æ–°å¢æ’­æ”¾ä¿®æ”¹å¾Œ input audio åŠŸèƒ½
#         # 3. å„²å­˜å¤šå€‹æŒ‰éˆ•ä»£è¡¨ä¸åŒéŸ³è¨Šï¼Œé¸æ“‡è¦åŒæ™‚æ’­æ”¾çš„å¹¾å€‹ï¼ˆæŒ‰ä¸‹å»ï¼‰ï¼Œå¾Œé»é¸æ’­æ”¾ï¼Œå‰‡åŒæ™‚æ’­æ”¾å°æ‡‰çš„å¹¾å€‹éŸ³è¨Š
#         with gr.Row():
#             aligned_input_playback = gr.Audio(label="æ‚¨çš„æ­Œè² (Input)", interactive=False, autoplay=False)
#             aligned_ref_playback = gr.Audio(label="åƒè€ƒéŸ³è¨Š (Reference)", interactive=False, autoplay=False)
#         gr.Markdown("*æ³¨æ„ï¼šGradio åƒ…æä¾›ä¸¦æ’æ’­æ”¾ï¼Œæ‚¨éœ€è¦æ‰‹å‹•åŒæ™‚é»æ“Šæ’­æ”¾ä»¥é€²è¡Œè½è¦ºä¸Šçš„åŒæ­¥æ¯”å°ã€‚DTW çµæœæ˜¯æ¼”ç®—æ³•ä¸Šå·²å°é½Šçš„ã€‚*")

#     # ç¶å®šäº‹ä»¶
#     # é»æ“ŠæŒ‰éˆ•å¾Œï¼Œå…ˆå°‡çµæœå€åŸŸè¨­ç‚ºéš±è—ï¼Œä»¥æ¸…é™¤èˆŠçš„çµæœ
#     def hide_results():
#         return gr.Column(visible=False)

#     analyze_btn.click(
#         fn=hide_results,
#         inputs=None,
#         outputs=[result_outputs_group]
#     ).then(
#         fn=singing_evaluator,
#         inputs=[input_audio_upload, ref_audio_upload],
#         outputs=[score_display, feedback_output, aligned_input_playback, aligned_ref_playback]
#     ).success(
#         fn=lambda: gr.Column(visible=True), # æˆåŠŸå¾Œå†é¡¯ç¤ºçµæœ
#         inputs=None,
#         outputs=result_outputs_group
#     )

# if __name__ == "__main__":
#     demo.launch(share=True)


import gradio as gr
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
import soundfile as sf
import tempfile
import os

# --- å…¨åŸŸè¨­å®š (Global Settings) ---
TARGET_SR = 16000 # è¨­å®šçµ±ä¸€çš„å–æ¨£ç‡ä»¥é€²è¡Œå…¬å¹³æ¯”è¼ƒ
HOP_LENGTH = 512 # Librosa é è¨­çš„ Frame Hop é•·åº¦

# æ ¸å¿ƒé‚è¼¯ 1: ç‰¹å¾µæå– (Feature Extraction)
def extract_features(audio_path):
    """è¼‰å…¥éŸ³è¨Šã€æ¨™æº–åŒ–è™•ç†ï¼Œä¸¦æå– Chroma å’Œ RMS ç‰¹å¾µã€‚"""
    if not audio_path or not os.path.exists(audio_path):
        raise gr.Error("è«‹éŒ„è£½æˆ–ä¸Šå‚³æœ‰æ•ˆçš„éŸ³è¨Šæª”æ¡ˆã€‚")

    try:
        # è¼‰å…¥éŸ³è¨Šï¼Œä¸¦é‡å–æ¨£è‡³ç›®æ¨™ SRï¼Œè½‰æ›ç‚ºå–®è²é“
        y, sr = librosa.load(audio_path, sr=TARGET_SR, mono=True)
    except Exception as e:
        raise gr.Error(f"è¼‰å…¥éŸ³è¨Šæª”æ¡ˆå¤±æ•—: {e}")

    # æª¢æŸ¥éŸ³è¨Šé•·åº¦æ˜¯å¦è¶³å¤ é€²è¡Œåˆ†æ
    MIN_SAMPLES = 2048 # FFT é‹ç®—éœ€è¦ä¸€å®šçš„æ¨£æœ¬æ•¸
    if len(y) < MIN_SAMPLES:
        raise gr.Error(f"éŸ³è¨Šé•·åº¦éçŸ­ ({len(y)/sr:.2f} ç§’)ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆåˆ†æã€‚")

    # 1. Chroma feature (éŸ³é«˜/å’Œè²å…§å®¹)
    chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=HOP_LENGTH)
    
    # 2. RMS (Root-Mean-Square Energy for volume)
    rms = librosa.feature.rms(y=y, hop_length=HOP_LENGTH)
    
    # åˆä½µç‰¹å¾µä¸¦è½‰ç½® -> (N_frames, 13)
    features = np.vstack([chroma, rms])
    
    return features.T

# æ ¸å¿ƒé‚è¼¯ 2: DTW å°é½Š (DTW Alignment)
def align_dtw(features_input, features_ref):
    """
    åŸ·è¡Œå‹•æ…‹æ™‚é–“è¦æ•´ (DTW) ä»¥å°é½Šå…©æ®µéŸ³è¨Šçš„æ™‚é–“è»¸ã€‚
    """
    # ä½¿ç”¨æ­å¹¾é‡Œå¾—è·é›¢ä½œç‚ºæˆæœ¬åº¦é‡
    D, wp = librosa.sequence.dtw(
        X=features_input.T,
        Y=features_ref.T,
        metric='euclidean'
    )
    return D, wp

# æ ¸å¿ƒé‚è¼¯ 3: çµæœåˆ†æèˆ‡å»ºè­°ç”Ÿæˆ (Analysis and Feedback Generation)
def analyze_results(wp, D, features_input, features_ref):
    """æ ¹æ“š DTW çµæœåˆ†æé€Ÿåº¦å’ŒéŸ³é«˜å·®ç•°ï¼Œä¸¦ç”Ÿæˆå»ºè­°ã€‚"""
    feedback = []
    
    # --- 1. æ•´é«”ç›¸ä¼¼åº¦åˆ†æ•¸ (Overall Similarity Score) ---
    # æ­£è¦åŒ– DTW æˆæœ¬ (å€¼è¶Šä½è¶Šç›¸ä¼¼)
    normalized_cost = D[-1, -1] / len(wp)
    
    # ã€å·²ä¿®æ”¹ã€‘å°‡æˆæœ¬è½‰æ›ç‚º 0-100 çš„åˆ†æ•¸ï¼Œè¶Šé«˜è¶Šå¥½
    # ä½¿ç”¨æŒ‡æ•¸è¡°æ¸›å‡½æ•¸ï¼Œk å€¼å¯èª¿æ•´è½‰æ›çš„éˆæ•åº¦
    k = 2.0 
    similarity_score = 100 * np.exp(-k * normalized_cost)
    
    # ã€æ–°å¢ã€‘åµæ¸¬æ˜¯å¦ç‚ºä¸åŒæ­Œæ›²
    # è¨­å®šä¸€å€‹ç¶“é©—é–¾å€¼ï¼Œè‹¥æˆæœ¬éé«˜ï¼Œå¯èƒ½ä»£è¡¨æ˜¯å®Œå…¨ä¸åŒçš„æ­Œæ›²
    DIFFERENT_SONG_THRESHOLD = 1.0 
    if normalized_cost > DIFFERENT_SONG_THRESHOLD:
        feedback.append(
            "**åˆ†æè­¦ç¤ºï¼š** å…©æ®µéŸ³è¨Šçš„å·®ç•°éå¤§ï¼Œç³»çµ±åˆ¤æ–·å¯èƒ½ä¾†è‡ª**ä¸åŒçš„æ­Œæ›²**ã€‚"
            "å› æ­¤ç›¸ä¼¼åº¦åˆ†æ•¸æ¥µä½ï¼Œä»¥ä¸‹çš„å±€éƒ¨å»ºè­°å¯èƒ½ä¸å…·åƒè€ƒåƒ¹å€¼ã€‚"
        )
        return f"{similarity_score:.1f}", '\n'.join(feedback)

    # --- 2. æ•´é«”ç¯€å¥/é€Ÿåº¦åˆ†æ (Global Tempo Analysis) ---
    input_frames = features_input.shape[0]
    ref_frames = features_ref.shape[0]
    avg_slope = input_frames / ref_frames
    
    tempo_suggestion = "**æ•´é«”ç¯€å¥è©•ä¼°:** "
    if avg_slope > 1.15:
        tempo_suggestion += f"æ‚¨çš„æ¼”å”±é€Ÿåº¦æ¯”åƒè€ƒéŸ³è¨Šæ…¢äº†ç´„ {avg_slope * 100 - 100:.0f}%ï¼Œå»ºè­°æ•´é«”åŠ å¿«ã€‚"
    elif avg_slope < 0.85:
        tempo_suggestion += f"æ‚¨çš„æ¼”å”±é€Ÿåº¦æ¯”åƒè€ƒéŸ³è¨Šå¿«äº†ç´„ {100 - avg_slope * 100:.0f}%ï¼Œå»ºè­°æ•´é«”æ”¾æ…¢ã€‚"
    else:
        tempo_suggestion += "æ‚¨çš„æ•´é«”ç¯€å¥æŒæ¡å¾—å¾ˆå¥½ï¼Œèˆ‡åƒè€ƒéŸ³è¨Šå¤§è‡´åŒæ­¥ã€‚"
    feedback.append(tempo_suggestion)
    
    # --- 3. å±€éƒ¨æ™‚åºèˆ‡éŸ³é«˜å•é¡Œ (Local Timing and Pitch Issues) ---
    feedback.append("\n**å…·é«”å±€éƒ¨æ”¹é€²å»ºè­°:**")
    
    # è¨ˆç®—è·¯å¾‘ä¸Šæ¯å€‹é»çš„å±€éƒ¨æˆæœ¬
    local_costs = np.array([D[i, j] for i, j in wp])
    # è¨ˆç®—æ¯ä¸€æ­¥çš„æˆæœ¬å¢é‡ï¼Œæ›´èƒ½åæ‡‰å•é¡Œé»
    step_costs = np.diff(local_costs, prepend=0)

    # æ‰¾å‡ºæˆæœ¬å¢é‡é¡¯è‘—é«˜æ–¼å¹³å‡å€¼çš„é»
    mean_step_cost = np.mean(step_costs)
    std_step_cost = np.std(step_costs)
    threshold = mean_step_cost + 1.5 * std_step_cost
    
    high_cost_indices = np.where(step_costs > threshold)[0]
    
    if high_cost_indices.size == 0:
        feedback.append("â€¢ è¡¨ç¾å„ªç•°ï¼æœªæª¢æ¸¬åˆ°æ˜é¡¯çš„å±€éƒ¨æ™‚åºæˆ–éŸ³é«˜å•é¡Œã€‚")
    else:
        # å°‡é€£çºŒçš„é«˜æˆæœ¬é»åˆ†çµ„ç‚ºå•é¡Œå€æ®µ
        issue_groups = []
        if high_cost_indices.size > 0:
            current_group = [high_cost_indices[0]]
            for i in range(1, len(high_cost_indices)):
                # å¦‚æœç´¢å¼•æ˜¯é€£çºŒçš„ï¼Œå‰‡è¦–ç‚ºåŒä¸€å•é¡Œ
                if high_cost_indices[i] == high_cost_indices[i-1] + 1:
                    current_group.append(high_cost_indices[i])
                else:
                    issue_groups.append(current_group)
                    current_group = [high_cost_indices[i]]
            issue_groups.append(current_group)
        
        # åˆ†ææ¯å€‹å•é¡Œå€æ®µ
        TIME_PER_FRAME = HOP_LENGTH / TARGET_SR
        for group in issue_groups[:5]: # æœ€å¤šé¡¯ç¤ºå‰ 5 å€‹å•é¡Œ
            start_k, end_k = group[0], group[-1]
            i_start, j_start = wp[start_k]
            i_end, j_end = wp[end_k]
            
            time_start = i_start * TIME_PER_FRAME
            time_end = i_end * TIME_PER_FRAME
            
            # æå–è©²å€æ®µçš„ Chroma ç‰¹å¾µ
            chroma_input_seg = features_input[i_start:i_end+1, :12]
            chroma_ref_seg = features_ref[j_start:j_end+1, :12]
            
            suggestion = ""
            if chroma_input_seg.size > 0 and chroma_ref_seg.size > 0:
                # è¨ˆç®—å±€éƒ¨é€Ÿåº¦å·®ç•°
                frames_input_seg = i_end - i_start
                frames_ref_seg = j_end - j_start
                local_slope = frames_input_seg / (frames_ref_seg + 1e-6)

                # è¨ˆç®—éŸ³é«˜å·®ç•°
                input_peak_bin = np.mean(np.argmax(chroma_input_seg, axis=1))
                ref_peak_bin = np.mean(np.argmax(chroma_ref_seg, axis=1))
                pitch_diff = input_peak_bin - ref_peak_bin
                
                if abs(pitch_diff) > 0.8: # éŸ³é«˜å·®ç•°é¡¯è‘—
                    pitch_level = "åé«˜" if pitch_diff > 0 else "åä½"
                    suggestion = f"éŸ³é«˜æ˜é¡¯**{pitch_level}** (åå·®ç´„ {abs(pitch_diff):.1f} å€‹åŠéŸ³)ã€‚"
                elif local_slope > 1.8:
                    suggestion = "**ç¯€å¥æ‹–æ²“**ï¼Œæ¼”å”±é€Ÿåº¦éæ…¢ã€‚"
                elif local_slope < 0.5:
                    suggestion = "**ç¯€å¥æ¶æ‹**ï¼Œæ¼”å”±é€Ÿåº¦éå¿«ã€‚"
                else:
                    suggestion = "éŸ³æº–æˆ–éŸ³è‰²ä¸åŒ¹é…ï¼Œè«‹æ³¨æ„æ­¤è™•çš„ç™¼è²ç©©å®šæ€§ã€‚"
            
            # ã€å·²ä¿®æ”¹ã€‘ä¿®å¾© "0 ç§’åˆ° 0 ç§’" å•é¡Œ
            if time_end - time_start < TIME_PER_FRAME:
                feedback.append(f"â€¢ **åœ¨ {time_start:.2f} ç§’é™„è¿‘:** {suggestion}")
            else:
                feedback.append(f"â€¢ **æ™‚é–“ {time_start:.2f} ç§’ - {time_end:.2f} ç§’:** {suggestion}")
        
        if len(issue_groups) > 5:
            feedback.append("â€¢ ... (åƒ…é¡¯ç¤ºå‰ 5 å€‹æœ€é¡¯è‘—çš„å•é¡Œé»)")

    return f"{similarity_score:.1f}", '\n'.join(feedback)

# æ ¸å¿ƒé‚è¼¯ 4: DTW å¯è¦–åŒ– (DTW Visualization)
def plot_dtw_path(D, wp):
    """ç¹ªè£½ç´¯ç©æˆæœ¬çŸ©é™£å’Œæœ€ä½³è¦æ•´è·¯å¾‘ã€‚"""
    fig = plt.figure(figsize=(8, 8))
    ax = fig.add_subplot(111)
    
    img = librosa.display.specshow(D, sr=TARGET_SR, x_axis='frames', y_axis='frames', ax=ax, hop_length=HOP_LENGTH)
    fig.colorbar(img, ax=ax, label='ç´¯ç©æˆæœ¬')
    ax.set(title='DTW ç´¯ç©æˆæœ¬èˆ‡æœ€ä½³è·¯å¾‘')
    
    # ç¹ªè£½æœ€ä½³è·¯å¾‘
    ax.plot(wp[:, 1], wp[:, 0], marker='.', color='red', linestyle='-', linewidth=2, alpha=0.6)
    
    ax.set_xlabel("åƒè€ƒéŸ³è¨Š (å½±æ ¼)")
    ax.set_ylabel("æ‚¨çš„æ­Œè² (å½±æ ¼)")
    plt.tight_layout()
    return fig

# ã€æ–°å¢ã€‘æ ¸å¿ƒé‚è¼¯ 5: æ··åˆéŸ³è¨Š (Mix Audio)
def mix_audio(path1, path2):
    """å°‡å…©æ®µéŸ³è¨Šæ··åˆæˆä¸€å€‹æª”æ¡ˆä»¥ä¾¿æ–¼æ¯”è¼ƒã€‚"""
    try:
        y1, _ = librosa.load(path1, sr=TARGET_SR, mono=True)
        y2, _ = librosa.load(path2, sr=TARGET_SR, mono=True)

        # å¡«å……è¼ƒçŸ­çš„éŸ³è¨Š
        len1, len2 = len(y1), len(y2)
        if len1 > len2:
            y2 = np.pad(y2, (0, len1 - len2))
        else:
            y1 = np.pad(y1, (0, len2 - len1))

        mixed = y1 + y2
        
        # æ­£è¦åŒ–ä»¥é˜²æ­¢å‰Šæ³¢
        max_amp = np.max(np.abs(mixed))
        if max_amp > 0:
            mixed /= max_amp
        
        # å„²å­˜åˆ°æš«å­˜æª”
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as fp:
            sf.write(fp.name, mixed, TARGET_SR)
            return fp.name
    except Exception as e:
        print(f"æ··åˆéŸ³è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None


# Gradio ä¸»è¦å‡½æ•¸
def singing_evaluator(input_audio_path, ref_audio_path):
    """Gradio ä»‹é¢çš„ä¸»è¦è™•ç†å‡½æ•¸ã€‚"""
    if not input_audio_path or not ref_audio_path:
        raise gr.Error("è«‹åŒæ™‚ä¸Šå‚³æˆ–éŒ„è£½æ‚¨çš„æ­Œè²å’Œåƒè€ƒéŸ³è¨Šã€‚")

    try:
        # 1. ç‰¹å¾µæå–
        features_input = extract_features(input_audio_path)
        features_ref = extract_features(ref_audio_path)
        
        # 2. DTW å°é½Š
        D, wp = align_dtw(features_input, features_ref)
        
        # 3. åˆ†æä¸¦ç”Ÿæˆå»ºè­°
        similarity_score, feedback_text = analyze_results(wp, D, features_input, features_ref)
        
        # 4. å¯è¦–åŒ– DTW è·¯å¾‘
        dtw_plot = plot_dtw_path(D, wp)

        # 5. æ··åˆéŸ³è¨Š
        mixed_audio_path = mix_audio(input_audio_path, ref_audio_path)
        
        # 6. è¿”å›æ‰€æœ‰çµæœ
        return similarity_score, feedback_text, dtw_plot, input_audio_path, ref_audio_path, mixed_audio_path
        
    except gr.Error as e:
        raise e # ç›´æ¥æ‹‹å‡º Gradio çš„éŒ¯èª¤
    except Exception as e:
        error_message = f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}"
        print(error_message) # åœ¨å¾Œç«¯æ—¥èªŒä¸­æ‰“å°è©³ç´°éŒ¯èª¤
        raise gr.Error("åˆ†æå¤±æ•—ï¼Œè«‹æª¢æŸ¥æ‚¨çš„éŸ³è¨Šæª”æ¡ˆæ˜¯å¦æœ‰æ•ˆï¼Œæˆ–ç¨å¾Œå†è©¦ã€‚")

# --- Gradio ç•Œé¢å®šç¾© ---
title = "ğŸ™ï¸ AI æ­Œè²ç›¸ä¼¼æ€§è©•ä¼°èˆ‡è¼”åŠ©ç³»çµ± ğŸ¶"
description = (
    "ä¸Šå‚³æˆ–**å³æ™‚éŒ„è£½**æ‚¨çš„æ­Œè²å’Œåƒè€ƒéŸ³è¨Šï¼Œç³»çµ±å°‡ä½¿ç”¨å‹•æ…‹æ™‚é–“è¦æ•´ (DTW) æŠ€è¡“ï¼Œ"
    "å¾**éŸ³æº–ã€ç¯€å¥**ç­‰å¤šå€‹ç¶­åº¦é€²è¡Œåˆ†æï¼Œæä¾› **0-100 çš„ç›¸ä¼¼åº¦åˆ†æ•¸**å’Œå…·é«”çš„æ”¹é€²å»ºè­°ã€‚"
    "æ‚¨é‚„å¯ä»¥æ’­æ”¾**ç–ŠåŠ å¾Œ**çš„éŸ³è¨Šï¼Œç›´è§€æ„Ÿå—å·®ç•°ã€‚"
)

with gr.Blocks(theme=gr.themes.Soft(), title=title) as demo:
    gr.Markdown(f"# {title}")
    gr.Markdown(description)

    with gr.Row():
        input_audio = gr.Audio(type="filepath", label="ğŸ¤ æ‚¨çš„æ­Œè² (Input)", sources=["upload", "microphone"])
        ref_audio = gr.Audio(type="filepath", label="ğŸ§ åƒè€ƒéŸ³è¨Š (Reference)", sources=["upload", "microphone"])
    
    analyze_btn = gr.Button("ğŸš€ é–‹å§‹åˆ†æèˆ‡è©•ä¼°", variant="primary")
    
    result_outputs_group = gr.Column(visible=False) 
    with result_outputs_group:
        gr.Markdown("---")
        gr.Markdown("## ğŸ“‹ è©•ä¼°å ±å‘Š")
        
        with gr.Row():
            score_display = gr.Textbox(label="ç¸½é«”ç›¸ä¼¼åº¦åˆ†æ•¸ (0-100åˆ†ï¼Œè¶Šé«˜è¶Šå¥½)", scale=1)
        
        feedback_output = gr.Markdown(label="### ğŸ“œ å…·é«”æ”¹é€²å»ºè­°")

        dtw_plot_output = gr.Plot(label="DTW è¦æ•´è·¯å¾‘åœ– (è¦–è¦ºåŒ–æ™‚é–“å°é½Š)")
        
        gr.Markdown("---")
        gr.Markdown("## ğŸ”Š éŸ³è¨Šæ¯”å°æ’­æ”¾")
        
        with gr.Row():
            input_playback = gr.Audio(label="æ‚¨çš„æ­Œè² (Input)")
            ref_playback = gr.Audio(label="åƒè€ƒéŸ³è¨Š (Reference)")
        
        mixed_playback = gr.Audio(label="ğŸ›ï¸ ç–ŠåŠ æ’­æ”¾ (Mixed for Comparison)")
        gr.Markdown("*æç¤ºï¼šæ’­æ”¾ã€Œç–ŠåŠ æ’­æ”¾ã€éŸ³è»Œï¼Œå¯ä»¥å¹«åŠ©æ‚¨æ›´æ¸…æ™°åœ°è½å‡ºç¯€å¥å’ŒéŸ³æº–çš„å·®ç•°ã€‚*")

    def run_analysis(input_audio_path, ref_audio_path):
        score, feedback, plot, _, _, mixed_path = singing_evaluator(input_audio_path, ref_audio_path)
        return {
            result_outputs_group: gr.Column(visible=True),
            score_display: score,
            feedback_output: feedback,
            dtw_plot_output: plot,
            input_playback: gr.Audio(value=input_audio_path, label="æ‚¨çš„æ­Œè² (Input)"),
            ref_playback: gr.Audio(value=ref_audio_path, label="åƒè€ƒéŸ³è¨Š (Reference)"),
            mixed_playback: gr.Audio(value=mixed_path, label="ğŸ›ï¸ ç–ŠåŠ æ’­æ”¾ (Mixed for Comparison)")
        }

    analyze_btn.click(
        fn=lambda: gr.Column(visible=False), # é»æ“Šå¾Œå…ˆéš±è—èˆŠçµæœ
        outputs=[result_outputs_group]
    ).then(
        fn=run_analysis,
        inputs=[input_audio, ref_audio],
        outputs=[result_outputs_group, score_display, feedback_output, dtw_plot_output, input_playback, ref_playback, mixed_playback]
    )

if __name__ == "__main__":
    demo.launch(share=True)

