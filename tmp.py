import gradio as gr
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import soundfile as sf
import tempfile
import os
import cv2
import threading
import time
from collections import deque
import subprocess

# ä¸­æ–‡å­—é«”è¨­å®š
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# å…¨åŸŸè¨­å®š
TARGET_SR = 16000
HOP_LENGTH = 512

def generate_pitch_animation_video(pitch_timeline, mixed_audio_path, output_path=None, progress=gr.Progress()):
    """
    ä½¿ç”¨ç´” matplotlib + ffmpeg ç”Ÿæˆå‹•ç•«å½±ç‰‡
    ç›¸å®¹æ–°ç‰ˆ matplotlibï¼Œç§»é™¤æ‰€æœ‰ emoji ç¬¦è™Ÿ
    """
    if not pitch_timeline:
        print("âš ï¸ ç„¡éŸ³é«˜æ™‚é–“è»¸æ•¸æ“šï¼Œè·³éå½±ç‰‡ç”Ÿæˆ")
        return None
    
    if output_path is None:
        output_path = tempfile.NamedTemporaryFile(suffix=".mp4", delete=False).name
    
    progress(0, desc="æº–å‚™ç”Ÿæˆå‹•ç•«å½±ç‰‡...")
    print("ğŸ“Š é–‹å§‹ç”Ÿæˆå‹•ç•«å½±ç‰‡...")
    
    # æå–æ•¸æ“š
    times = np.array([d['time'] for d in pitch_timeline])
    pitch_diffs = np.array([d['pitch_diff'] for d in pitch_timeline])
    similarities = np.array([d['similarity'] for d in pitch_timeline])
    
    duration = times[-1]
    fps = 20
    
    progress(0.1, desc="å‰µå»ºå‹•ç•«åœ–å½¢...")
    # å‰µå»ºåœ–å½¢
    fig = plt.figure(figsize=(14, 10))
    
    def animate(frame):
        """å‹•ç•«æ›´æ–°å‡½æ•¸ï¼ˆç§»é™¤æ‰€æœ‰ emojiï¼‰"""
        current_time = frame / fps
        fig.clear()
        
        gs = fig.add_gridspec(3, 1, height_ratios=[1, 2, 2], hspace=0.3)
        
        # === é ‚éƒ¨ï¼šå³æ™‚ç‹€æ…‹é¡¯ç¤º ===
        ax_status = fig.add_subplot(gs[0])
        ax_status.axis('off')
        
        if len(times) > 0:
            idx = np.argmin(np.abs(times - current_time))
            current_data = pitch_timeline[idx]
            
            pitch_diff = current_data['pitch_diff']
            similarity = current_data['similarity']
            
            # ç‹€æ…‹åˆ¤æ–·ï¼ˆæ”¹ç”¨ç´”æ–‡å­—æ¨™ç±¤ï¼‰
            if abs(pitch_diff) < 0.5:
                status = "[æº–ç¢º] éŸ³æº–æº–ç¢º"
                status_color = 'green'
            elif abs(pitch_diff) < 1.5:
                status = "[æ³¨æ„] è¼•å¾®åå·®"
                status_color = 'orange'
            else:
                status = "[è­¦å‘Š] æ˜é¡¯åå·®"
                status_color = 'red'
            
            direction = "åé«˜" if pitch_diff > 0 else "åä½" if pitch_diff < 0 else "æº–ç¢º"
            
            # é¡¯ç¤ºç‹€æ…‹ï¼ˆä½¿ç”¨çŸ©å½¢èƒŒæ™¯çªé¡¯ï¼‰
            bbox_props = dict(boxstyle='round,pad=0.5', facecolor=status_color, alpha=0.2)
            status_text = f"{status}\nç•¶å‰æ™‚é–“: {current_time:.2f} ç§’"
            ax_status.text(0.5, 0.7, status_text, 
                          fontsize=18, fontweight='bold', 
                          ha='center', va='center', 
                          color=status_color, bbox=bbox_props)
            
            detail_text = (f"éŸ³é«˜åå·®: {abs(pitch_diff):.2f} åŠéŸ³ ({direction}) | "
                          f"ç›¸ä¼¼åº¦: {similarity:.1f} åˆ†")
            ax_status.text(0.5, 0.3, detail_text,
                          fontsize=11, ha='center', va='center', color='black')
        
        # === éŸ³é«˜åå·®è¿½è¹¤åœ– ===
        ax_pitch = fig.add_subplot(gs[1])
        window_size = 10.0
        x_min = max(0, current_time - window_size / 2)
        x_max = current_time + window_size / 2
        
        mask_played = times <= current_time
        mask_future = times > current_time
        
        colors_played = ['red' if abs(pd) > 1.5 else 'orange' if abs(pd) > 0.5 else 'green' 
                         for pd in pitch_diffs[mask_played]]
        
        # ç¹ªè£½å·²æ’­æ”¾éƒ¨åˆ†ï¼ˆé«˜äº®ï¼‰
        if np.any(mask_played):
            ax_pitch.scatter(times[mask_played], pitch_diffs[mask_played], 
                            c=colors_played, alpha=0.8, s=50, zorder=3, edgecolors='black', linewidth=0.5)
            ax_pitch.plot(times[mask_played], pitch_diffs[mask_played], 
                         color='gray', alpha=0.5, linewidth=2, zorder=2)
        
        # ç¹ªè£½æœªæ’­æ”¾éƒ¨åˆ†ï¼ˆç°è‰²ï¼‰
        if np.any(mask_future):
            ax_pitch.scatter(times[mask_future], pitch_diffs[mask_future], 
                            c='lightgray', alpha=0.3, s=30, zorder=1)
        
        # ç•¶å‰æ’­æ”¾ä½ç½®æ¨™è¨˜
        ax_pitch.axvline(x=current_time, color='blue', linestyle='-', linewidth=3, zorder=4)
        
        # æ¨™è¨˜å®‰å…¨å€åŸŸ
        ax_pitch.axhspan(-0.5, 0.5, alpha=0.15, color='green', zorder=0)
        ax_pitch.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)
        
        ax_pitch.set_xlabel('æ™‚é–“ (ç§’)', fontsize=11)
        ax_pitch.set_ylabel('éŸ³é«˜åå·® (åŠéŸ³)', fontsize=11)
        ax_pitch.set_title('å³æ™‚éŸ³é«˜åå·®è¿½è¹¤', fontsize=13, fontweight='bold')
        ax_pitch.grid(True, alpha=0.3)
        ax_pitch.set_ylim(-6, 6)
        ax_pitch.set_xlim(x_min, x_max)
        
        # === ç›¸ä¼¼åº¦è¿½è¹¤åœ– ===
        ax_similarity = fig.add_subplot(gs[2])
        
        # ç¹ªè£½å·²æ’­æ”¾éƒ¨åˆ†
        if np.any(mask_played):
            ax_similarity.plot(times[mask_played], similarities[mask_played], 
                              color='#2E86AB', linewidth=3, zorder=3)
            ax_similarity.fill_between(times[mask_played], similarities[mask_played], 0,
                                      alpha=0.3, color='#2E86AB', zorder=2)
        
        # ç¹ªè£½æœªæ’­æ”¾éƒ¨åˆ†
        if np.any(mask_future):
            ax_similarity.plot(times[mask_future], similarities[mask_future], 
                              color='lightgray', linewidth=2, alpha=0.5, zorder=1)
        
        # ç•¶å‰æ’­æ”¾ä½ç½®æ¨™è¨˜
        ax_similarity.axvline(x=current_time, color='blue', linestyle='-', linewidth=3, zorder=4)
        ax_similarity.axhline(y=70, color='gray', linestyle='--', alpha=0.5)
        ax_similarity.fill_between([x_min, x_max], 70, 100, alpha=0.1, color='green')
        ax_similarity.fill_between([x_min, x_max], 0, 70, alpha=0.1, color='red')
        
        ax_similarity.set_xlabel('æ™‚é–“ (ç§’)', fontsize=11)
        ax_similarity.set_ylabel('ç›¸ä¼¼åº¦åˆ†æ•¸', fontsize=11)
        ax_similarity.set_title('å³æ™‚éŸ³æº–ç›¸ä¼¼åº¦', fontsize=13, fontweight='bold')
        ax_similarity.grid(True, alpha=0.3)
        ax_similarity.set_ylim(0, 105)
        ax_similarity.set_xlim(x_min, x_max)
    
    # ç”Ÿæˆå‹•ç•«
    total_frames = int(duration * fps)
    anim = animation.FuncAnimation(fig, animate, frames=total_frames, 
                                   interval=1000/fps, blit=False)
    
    # ä¿å­˜ç„¡éŸ³è¨Šçš„å½±ç‰‡
    temp_video = output_path.replace('.mp4', '_no_audio.mp4')
    
    try:
        progress(0.3, desc="æ­£åœ¨æ¸²æŸ“å‹•ç•«å¹€...")
        print("â³ æ­£åœ¨æ¸²æŸ“å‹•ç•«å¹€ï¼ˆé€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ï¼‰...")
        writer = animation.FFMpegWriter(fps=fps, bitrate=1800, codec='libx264')
        anim.save(temp_video, writer=writer, dpi=100)
        plt.close(fig)
        
        progress(0.8, desc="æ­£åœ¨åˆä½µéŸ³è¨Š...")
        print("ğŸµ æ­£åœ¨åˆä½µéŸ³è¨Š...")
        # ä½¿ç”¨ ffmpeg åˆä½µéŸ³è¨Š
        result = subprocess.run([
            'ffmpeg', '-y', '-loglevel', 'error',
            '-i', temp_video,
            '-i', mixed_audio_path,
            '-c:v', 'copy',
            '-c:a', 'aac',
            '-shortest',
            output_path
        ], capture_output=True, text=True)
        
        if result.returncode != 0:
            print(f"âš ï¸ ffmpeg è­¦å‘Š: {result.stderr}")
        
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        if os.path.exists(temp_video):
            os.remove(temp_video)
        
        progress(1.0, desc="å‹•ç•«å½±ç‰‡ç”Ÿæˆå®Œæˆï¼")
        print(f"âœ… å‹•ç•«å½±ç‰‡ç”Ÿæˆå®Œæˆï¼")
        return output_path
        
    except FileNotFoundError:
        print("âŒ éŒ¯èª¤ï¼šç³»çµ±æœªå®‰è£ ffmpeg")
        print("è«‹åŸ·è¡Œï¼šbrew install ffmpeg")
        return None
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå½±ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        # å¦‚æœåˆä½µå¤±æ•—ï¼Œè¿”å›ç„¡éŸ³è¨Šç‰ˆæœ¬
        if os.path.exists(temp_video):
            print(f"âš ï¸ è¿”å›ç„¡éŸ³è¨Šç‰ˆæœ¬: {temp_video}")
            return temp_video
        return None

# --- å³æ™‚æ’­æ”¾åˆ†æå…¨åŸŸè®Šæ•¸ ---
class PlaybackState:
    """æ’­æ”¾ç‹€æ…‹ç®¡ç†é¡"""
    def __init__(self):
        self.is_playing = False
        self.current_time = 0.0
        self.total_duration = 0.0
        self.pitch_data = []
        self.lock = threading.Lock()
        
playback_state = PlaybackState()

# æ ¸å¿ƒé‚è¼¯ 7: é è¨ˆç®—éŸ³é«˜å·®ç•°æ•¸æ“š (Pre-compute Pitch Difference Data)
def precompute_pitch_differences(features_input, features_ref, interval=0.1, progress=gr.Progress()):
    """
    é å…ˆè¨ˆç®—æ•´æ®µéŸ³è¨Šæ¯å€‹æ™‚é–“é»çš„éŸ³é«˜å·®ç•°ã€‚
    """
    progress(0, desc="é–‹å§‹é è¨ˆç®—éŸ³é«˜æ•¸æ“š...")
    
    frames_per_interval = int(interval * TARGET_SR / HOP_LENGTH)
    min_frames = min(features_input.shape[0], features_ref.shape[0])
    
    pitch_timeline = []
    total_intervals = max(1, min_frames // frames_per_interval)
    
    for idx, frame_idx in enumerate(progress.tqdm(range(0, min_frames, frames_per_interval), 
                                                   desc="è¨ˆç®—éŸ³é«˜å·®ç•°")):
        # å–ä¸€å°æ®µç‰¹å¾µé€²è¡Œåˆ†æï¼ˆä½¿ç”¨ 0.5 ç§’çš„è¦–çª—ï¼‰
        window_size = int(0.5 * TARGET_SR / HOP_LENGTH)
        end_idx = min(frame_idx + window_size, min_frames)
        
        if end_idx - frame_idx < 10:  # è¦–çª—å¤ªå°å°±è·³é
            continue
        
        # æå– Chroma ç‰¹å¾µ
        chroma_input = features_input[frame_idx:end_idx, :12]
        chroma_ref = features_ref[frame_idx:end_idx, :12]
        
        # è¨ˆç®—å¹³å‡éŸ³é«˜
        input_pitch = np.argmax(np.mean(chroma_input, axis=0))
        ref_pitch = np.argmax(np.mean(chroma_ref, axis=0))
        
        # è¨ˆç®—éŸ³é«˜åå·®ï¼ˆåŠéŸ³ï¼‰
        pitch_diff = (input_pitch - ref_pitch) % 12
        if pitch_diff > 6:
            pitch_diff = pitch_diff - 12
        
        # è¨ˆç®—éŸ³æº–ç›¸ä¼¼åº¦
        chroma_input_norm = chroma_input / (np.linalg.norm(chroma_input, axis=1, keepdims=True) + 1e-8)
        chroma_ref_norm = chroma_ref / (np.linalg.norm(chroma_ref, axis=1, keepdims=True) + 1e-8)
        similarity = np.mean(np.sum(chroma_input_norm * chroma_ref_norm, axis=1)) * 100
        
        # è¨ˆç®—æ™‚é–“æˆ³
        timestamp = frame_idx * HOP_LENGTH / TARGET_SR
        
        pitch_timeline.append({
            'time': timestamp,
            'pitch_diff': pitch_diff,
            'similarity': similarity,
            'input_pitch': input_pitch,
            'ref_pitch': ref_pitch
        })
    
    return pitch_timeline


# æ ¸å¿ƒé‚è¼¯ 8: å³æ™‚æ’­æ”¾æ§åˆ¶èˆ‡è¦–è¦ºåŒ–æ›´æ–° (Real-time Playback Control)
def update_realtime_display(pitch_timeline, current_time):
    """
    æ ¹æ“šç•¶å‰æ’­æ”¾æ™‚é–“æ›´æ–°å³æ™‚éŸ³é«˜é¡¯ç¤ºã€‚
    """
    if not pitch_timeline:
        return "æš«ç„¡æ•¸æ“š", None
    
    # æ‰¾åˆ°ç•¶å‰æ™‚é–“æœ€æ¥è¿‘çš„æ•¸æ“šé»
    closest_data = min(pitch_timeline, key=lambda x: abs(x['time'] - current_time))
    
    # æ ¼å¼åŒ–é¡¯ç¤ºæ–‡æœ¬
    pitch_diff = closest_data['pitch_diff']
    similarity = closest_data['similarity']
    
    if abs(pitch_diff) < 0.5:
        status = "âœ… éŸ³æº–æº–ç¢º"
        color = "ğŸŸ¢"
    elif abs(pitch_diff) < 1.5:
        status = "âš ï¸ è¼•å¾®åå·®"
        color = "ğŸŸ¡"
    else:
        status = "âŒ æ˜é¡¯åå·®"
        color = "ğŸ”´"
    
    direction = "åé«˜" if pitch_diff > 0 else "åä½" if pitch_diff < 0 else "æº–ç¢º"
    
    display_text = f"""
### {color} å³æ™‚éŸ³é«˜åˆ†æ ({current_time:.2f} ç§’)

**ç‹€æ…‹**: {status}  
**éŸ³é«˜åå·®**: {abs(pitch_diff):.2f} åŠéŸ³ ({direction})  
**éŸ³æº–ç›¸ä¼¼åº¦**: {similarity:.1f} åˆ†  

---
**éŸ³ç¬¦å°ç…§**:
- æ‚¨çš„éŸ³é«˜: {get_note_name(closest_data['input_pitch'])}
- åƒè€ƒéŸ³é«˜: {get_note_name(closest_data['ref_pitch'])}
"""
    
    # ç”Ÿæˆå³æ™‚åœ–è¡¨
    plot_fig = plot_realtime_pitch(pitch_timeline, current_time)
    
    return display_text, plot_fig


def get_note_name(pitch_class):
    """å°‡éŸ³é«˜é¡åˆ¥è½‰æ›ç‚ºéŸ³ç¬¦åç¨±"""
    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    return note_names[int(pitch_class) % 12]


def plot_realtime_pitch(pitch_timeline, current_time, window_size=10.0):
    """
    ç¹ªè£½å³æ™‚éŸ³é«˜å·®ç•°åœ–è¡¨ï¼Œé¡¯ç¤ºç•¶å‰æ’­æ”¾ä½ç½®ã€‚
    """
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
    if not pitch_timeline:
        ax1.text(0.5, 0.5, 'æš«ç„¡æ•¸æ“š', ha='center', va='center', fontsize=14)
        ax2.text(0.5, 0.5, 'æš«ç„¡æ•¸æ“š', ha='center', va='center', fontsize=14)
        plt.tight_layout()
        return fig
    
    times = [d['time'] for d in pitch_timeline]
    pitch_diffs = [d['pitch_diff'] for d in pitch_timeline]
    similarities = [d['similarity'] for d in pitch_timeline]
    
    # åœ–è¡¨ 1: éŸ³é«˜åå·®
    colors = ['red' if abs(pd) > 1.5 else 'orange' if abs(pd) > 0.5 else 'green' 
              for pd in pitch_diffs]
    
    ax1.scatter(times, pitch_diffs, c=colors, alpha=0.6, s=30)
    ax1.plot(times, pitch_diffs, color='gray', alpha=0.3, linewidth=1)
    
    # æ¨™è¨˜ç•¶å‰æ’­æ”¾ä½ç½®
    ax1.axvline(x=current_time, color='blue', linestyle='--', linewidth=2, label='ç•¶å‰æ’­æ”¾ä½ç½®')
    
    # æ¨™è¨˜å®‰å…¨å€åŸŸ
    ax1.axhspan(-0.5, 0.5, alpha=0.2, color='green', label='æº–ç¢ºç¯„åœ')
    ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)
    
    ax1.set_xlabel('æ™‚é–“ (ç§’)', fontsize=11)
    ax1.set_ylabel('éŸ³é«˜åå·® (åŠéŸ³)', fontsize=11)
    ax1.set_title('å³æ™‚éŸ³é«˜åå·®è¿½è¹¤', fontsize=13, fontweight='bold')
    ax1.legend(loc='upper right', fontsize=9)
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(-6, 6)
    
    # è¨­å®š x è»¸ç¯„åœï¼ˆé¡¯ç¤ºç•¶å‰ä½ç½®å‰å¾Œçš„è¦–çª—ï¼‰
    x_min = max(0, current_time - window_size / 2)
    x_max = current_time + window_size / 2
    ax1.set_xlim(x_min, x_max)
    
    # åœ–è¡¨ 2: éŸ³æº–ç›¸ä¼¼åº¦
    ax2.plot(times, similarities, color='#2E86AB', linewidth=2, label='éŸ³æº–ç›¸ä¼¼åº¦')
    ax2.fill_between(times, 70, 100, alpha=0.1, color='green')
    ax2.fill_between(times, 0, 70, alpha=0.1, color='red')
    
    # æ¨™è¨˜ç•¶å‰æ’­æ”¾ä½ç½®
    ax2.axvline(x=current_time, color='blue', linestyle='--', linewidth=2, label='ç•¶å‰æ’­æ”¾ä½ç½®')
    ax2.axhline(y=70, color='gray', linestyle='--', alpha=0.5, label='è‰¯å¥½é–€æª»')
    
    ax2.set_xlabel('æ™‚é–“ (ç§’)', fontsize=11)
    ax2.set_ylabel('ç›¸ä¼¼åº¦åˆ†æ•¸ (0-100)', fontsize=11)
    ax2.set_title('å³æ™‚éŸ³æº–ç›¸ä¼¼åº¦', fontsize=13, fontweight='bold')
    ax2.legend(loc='lower right', fontsize=9)
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0, 105)
    ax2.set_xlim(x_min, x_max)
    
    plt.tight_layout()
    return fig

# æ ¸å¿ƒé‚è¼¯ 5: è¦–çª—å¼éŸ³æº–åˆ†æ (Windowed Pitch Analysis)
def windowed_pitch_analysis(features_input, features_ref, window_size=5.0, overlap=2.0, progress=gr.Progress()):
    """
    ä½¿ç”¨æ»‘å‹•è¦–çª—åˆ†æéŸ³æº–ï¼Œæ¯å€‹è¦–çª—ç¨ç«‹é€²è¡Œ DTW å’ŒéŸ³æº–è©•ä¼°ã€‚
    """
    progress(0, desc="é–‹å§‹è¦–çª—å¼åˆ†æ...")
    
    # è¨ˆç®—æ¯å€‹è¦–çª—çš„å¹€æ•¸
    frames_per_window = int(window_size * TARGET_SR / HOP_LENGTH)
    frames_per_step = int((window_size - overlap) * TARGET_SR / HOP_LENGTH)
    
    results = []
    
    total_input_frames = features_input.shape[0]
    total_ref_frames = features_ref.shape[0]
    
    # ä½¿ç”¨è¼ƒçŸ­çš„éŸ³è¨Šé•·åº¦ä½œç‚ºåŸºæº–
    min_frames = min(total_input_frames, total_ref_frames)
    
    window_start = 0
    window_idx = 0
    
    # è¨ˆç®—ç¸½è¦–çª—æ•¸
    total_windows = max(1, (min_frames - frames_per_window) // frames_per_step + 1)
    
    while window_start + frames_per_window <= min_frames:
        progress(window_idx / total_windows, desc=f"åˆ†æè¦–çª— {window_idx+1}/{total_windows}")
        
        window_end = window_start + frames_per_window
        
        # æå–ç•¶å‰è¦–çª—çš„ç‰¹å¾µ
        input_window = features_input[window_start:window_end]
        ref_window = features_ref[window_start:window_end]
        
        # æª¢æŸ¥è¦–çª—å¤§å°
        if input_window.shape[0] < 10 or ref_window.shape[0] < 10:
            window_start += frames_per_step
            continue
        
        try:
            # å°è¦–çª—é€²è¡Œ DTW å°é½Š
            D, wp = align_dtw(input_window, ref_window)
            
            # æå– Chroma ç‰¹å¾µï¼ˆå‰ 12 ç¶­ï¼‰
            chroma_input = input_window[:, :12]
            chroma_ref = ref_window[:, :12]
            
            # è¨ˆç®—éŸ³æº–åˆ†æ•¸å’Œåå·®
            pitch_score, pitch_deviation, pitch_direction = calculate_window_pitch_score(
                chroma_input, chroma_ref, D, wp
            )
            
            # è¨ˆç®—ç¯€å¥åˆ†æ•¸
            tempo_score = calculate_tempo_score(len(input_window), len(ref_window))
            
            # æ•´é«”è¦–çª—åˆ†æ•¸
            overall_score = 0.7 * pitch_score + 0.3 * tempo_score
            
            # è¨˜éŒ„æ™‚é–“è³‡è¨Š
            time_start = window_start * HOP_LENGTH / TARGET_SR
            time_end = window_end * HOP_LENGTH / TARGET_SR
            
            results.append({
                'window_idx': window_idx,
                'time_start': time_start,
                'time_end': time_end,
                'time_center': (time_start + time_end) / 2,
                'pitch_score': pitch_score,
                'pitch_deviation': pitch_deviation,
                'pitch_direction': pitch_direction,
                'tempo_score': tempo_score,
                'overall_score': overall_score,
                'normalized_dtw_cost': D[-1, -1] / len(wp)
            })
            
        except Exception as e:
            print(f"è¦–çª— {window_idx} åˆ†æå¤±æ•—: {e}")
        
        window_start += frames_per_step
        window_idx += 1
    
    return results


def calculate_window_pitch_score(chroma_input, chroma_ref, D, wp):
    """
    è¨ˆç®—è¦–çª—å…§çš„éŸ³æº–åˆ†æ•¸å’Œåå·®ã€‚
    """
    # è¨ˆç®—å¹³å‡éŸ³é«˜
    input_peak_bins = np.argmax(chroma_input, axis=1)
    ref_peak_bins = np.argmax(chroma_ref, axis=1)
    
    # è¨ˆç®—éŸ³é«˜åå·®
    pitch_diff = np.mean(input_peak_bins) - np.mean(ref_peak_bins)
    
    # è¨ˆç®—éŸ³æº–ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨é¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰
    chroma_input_norm = chroma_input / (np.linalg.norm(chroma_input, axis=1, keepdims=True) + 1e-8)
    chroma_ref_norm = chroma_ref / (np.linalg.norm(chroma_ref, axis=1, keepdims=True) + 1e-8)
    
    cosine_similarities = np.sum(chroma_input_norm * chroma_ref_norm, axis=1)
    avg_similarity = np.mean(cosine_similarities)
    
    # è½‰æ›ç‚ºåˆ†æ•¸ (0-100)
    pitch_score = max(0, min(100, avg_similarity * 100))
    
    # åˆ¤æ–·åå·®æ–¹å‘
    if abs(pitch_diff) < 0.5:
        pitch_direction = "æº–ç¢º"
    elif pitch_diff > 0:
        pitch_direction = "åé«˜"
    else:
        pitch_direction = "åä½"
    
    return pitch_score, abs(pitch_diff), pitch_direction


def calculate_tempo_score(input_frames, ref_frames):
    """è¨ˆç®—ç¯€å¥åˆ†æ•¸"""
    tempo_ratio = input_frames / (ref_frames + 1e-8)
    
    # ç†æƒ³æ¯”ä¾‹ç‚º 1ï¼Œåé›¢è¶Šå¤šåˆ†æ•¸è¶Šä½
    tempo_deviation = abs(tempo_ratio - 1.0)
    tempo_score = max(0, 100 * (1 - tempo_deviation * 2))
    
    return tempo_score


# æ ¸å¿ƒé‚è¼¯ 6: è¦–çª—åˆ†æçµæœå¯è¦–åŒ– (Windowed Analysis Visualization)
def plot_windowed_analysis(results):
    """
    ç¹ªè£½è¦–çª—åˆ†æçµæœçš„æ™‚é–“åºåˆ—åœ–ã€‚
    """
    if not results:
        fig = plt.figure(figsize=(12, 6))
        plt.text(0.5, 0.5, 'ç„¡è¶³å¤ è³‡æ–™é€²è¡Œè¦–çª—åˆ†æ', 
                ha='center', va='center', fontsize=14)
        return fig
    
    times = [r['time_center'] for r in results]
    pitch_scores = [r['pitch_score'] for r in results]
    tempo_scores = [r['tempo_score'] for r in results]
    overall_scores = [r['overall_score'] for r in results]
    pitch_deviations = [r['pitch_deviation'] for r in results]
    
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    
    # ç¬¬ä¸€å¼µåœ–ï¼šåˆ†æ•¸éš¨æ™‚é–“è®ŠåŒ–
    ax1 = axes[0]
    ax1.plot(times, pitch_scores, marker='o', label='éŸ³æº–åˆ†æ•¸', linewidth=2, color='#2E86AB')
    ax1.plot(times, tempo_scores, marker='s', label='ç¯€å¥åˆ†æ•¸', linewidth=2, color='#A23B72')
    ax1.plot(times, overall_scores, marker='^', label='æ•´é«”åˆ†æ•¸', linewidth=2.5, color='#F18F01')
    
    ax1.axhline(y=70, color='gray', linestyle='--', alpha=0.5, label='è‰¯å¥½é–€æª» (70åˆ†)')
    ax1.fill_between(times, 70, 100, alpha=0.1, color='green')
    ax1.fill_between(times, 0, 70, alpha=0.1, color='red')
    
    ax1.set_xlabel('æ™‚é–“ (ç§’)', fontsize=12)
    ax1.set_ylabel('åˆ†æ•¸ (0-100)', fontsize=12)
    ax1.set_title('å„æ™‚æ®µéŸ³æº–èˆ‡ç¯€å¥è©•ä¼°', fontsize=14, fontweight='bold')
    ax1.legend(loc='lower left', fontsize=10)
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 105)
    
    # ç¬¬äºŒå¼µåœ–ï¼šéŸ³é«˜åå·®
    ax2 = axes[1]
    colors = ['red' if r['pitch_direction'] == 'åé«˜' else 'blue' if r['pitch_direction'] == 'åä½' else 'green' 
              for r in results]
    
    bars = ax2.bar(times, pitch_deviations, width=2, color=colors, alpha=0.7, edgecolor='black')
    ax2.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='å¯æ¥å—åå·® (0.5åŠéŸ³)')
    
    ax2.set_xlabel('æ™‚é–“ (ç§’)', fontsize=12)
    ax2.set_ylabel('éŸ³é«˜åå·® (åŠéŸ³)', fontsize=12)
    ax2.set_title('å„æ™‚æ®µéŸ³é«˜åå·®åˆ†æ', fontsize=14, fontweight='bold')
    ax2.legend(loc='upper right', fontsize=10)
    ax2.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ é¡è‰²åœ–ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='red', alpha=0.7, label='åé«˜'),
        Patch(facecolor='blue', alpha=0.7, label='åä½'),
        Patch(facecolor='green', alpha=0.7, label='æº–ç¢º')
    ]
    ax2.legend(handles=legend_elements, loc='upper left', fontsize=10)
    
    plt.tight_layout()
    return fig


def generate_windowed_feedback(results):
    """
    æ ¹æ“šè¦–çª—åˆ†æçµæœç”Ÿæˆæ–‡å­—å»ºè­°ã€‚
    """
    if not results:
        return "éŸ³è¨Šé•·åº¦ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œè¦–çª—åˆ†æã€‚"
    
    feedback = []
    
    # æ•´é«”çµ±è¨ˆ
    avg_pitch_score = np.mean([r['pitch_score'] for r in results])
    avg_tempo_score = np.mean([r['tempo_score'] for r in results])
    avg_overall = np.mean([r['overall_score'] for r in results])
    
    feedback.append(f"### ğŸ“Š æ•´é«”è¡¨ç¾çµ±è¨ˆ")
    feedback.append(f"- **å¹³å‡éŸ³æº–åˆ†æ•¸**: {avg_pitch_score:.1f} åˆ†")
    feedback.append(f"- **å¹³å‡ç¯€å¥åˆ†æ•¸**: {avg_tempo_score:.1f} åˆ†")
    feedback.append(f"- **å¹³å‡æ•´é«”åˆ†æ•¸**: {avg_overall:.1f} åˆ†")
    feedback.append("")
    
    # æ‰¾å‡ºå•é¡Œæ™‚æ®µï¼ˆåˆ†æ•¸ä½æ–¼ 70 çš„ï¼‰
    problem_windows = [r for r in results if r['overall_score'] < 70]
    
    if problem_windows:
        feedback.append(f"### âš ï¸ éœ€è¦æ”¹é€²çš„æ™‚æ®µï¼ˆå…± {len(problem_windows)} å€‹ï¼‰")
        for i, window in enumerate(problem_windows[:5], 1):  # æœ€å¤šé¡¯ç¤º 5 å€‹
            time_range = f"{window['time_start']:.1f}ç§’ - {window['time_end']:.1f}ç§’"
            
            issues = []
            if window['pitch_score'] < 70:
                issues.append(f"éŸ³æº–{window['pitch_direction']}ï¼ˆåå·® {window['pitch_deviation']:.1f} åŠéŸ³ï¼‰")
            if window['tempo_score'] < 70:
                issues.append("ç¯€å¥ä¸ç©©")
            
            feedback.append(f"{i}. **{time_range}**: {', '.join(issues)}")
        
        if len(problem_windows) > 5:
            feedback.append(f"   ... é‚„æœ‰ {len(problem_windows) - 5} å€‹æ™‚æ®µéœ€è¦æ³¨æ„")
    else:
        feedback.append("### âœ… è¡¨ç¾å„ªç•°")
        feedback.append("æ‰€æœ‰æ™‚æ®µçš„è¡¨ç¾éƒ½åœ¨è‰¯å¥½æ°´æº–ä»¥ä¸Šï¼Œè«‹ç¹¼çºŒä¿æŒï¼")
    
    feedback.append("")
    
    # æœ€ä½³æ™‚æ®µ
    best_window = max(results, key=lambda r: r['overall_score'])
    feedback.append(f"### ğŸŒŸ æœ€ä½³æ™‚æ®µ")
    feedback.append(f"**{best_window['time_start']:.1f}ç§’ - {best_window['time_end']:.1f}ç§’** "
                   f"(æ•´é«”åˆ†æ•¸: {best_window['overall_score']:.1f})")
    
    return '\n'.join(feedback)


# æ ¸å¿ƒé‚è¼¯ 1: ç‰¹å¾µæå– (Feature Extraction)
def extract_features(audio_path, progress=gr.Progress()):
    """è¼‰å…¥éŸ³è¨Šã€æ¨™æº–åŒ–è™•ç†ï¼Œä¸¦æå– Chroma å’Œ RMS ç‰¹å¾µã€‚"""
    if not audio_path or not os.path.exists(audio_path):
        raise gr.Error("è«‹éŒ„è£½æˆ–ä¸Šå‚³æœ‰æ•ˆçš„éŸ³è¨Šæª”æ¡ˆã€‚")

    try:
        progress(0, desc="è¼‰å…¥éŸ³è¨Šæª”æ¡ˆ...")
        # è¼‰å…¥éŸ³è¨Šï¼Œä¸¦é‡å–æ¨£è‡³ç›®æ¨™ SRï¼Œè½‰æ›ç‚ºå–®è²é“
        y, sr = librosa.load(audio_path, sr=TARGET_SR, mono=True)
    except Exception as e:
        raise gr.Error(f"è¼‰å…¥éŸ³è¨Šæª”æ¡ˆå¤±æ•—: {e}")

    # æª¢æŸ¥éŸ³è¨Šé•·åº¦æ˜¯å¦è¶³å¤ é€²è¡Œåˆ†æ
    MIN_SAMPLES = 2048
    if len(y) < MIN_SAMPLES:
        raise gr.Error(f"éŸ³è¨Šé•·åº¦éçŸ­ ({len(y)/sr:.2f} ç§’)ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆåˆ†æã€‚")

    progress(0.3, desc="æå– Chroma ç‰¹å¾µ...")
    # 1. Chroma feature (éŸ³é«˜/å’Œè²å…§å®¹)
    chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=HOP_LENGTH)
    
    progress(0.7, desc="æå– RMS ç‰¹å¾µ...")
    # 2. RMS (Root-Mean-Square Energy for volume)
    rms = librosa.feature.rms(y=y, hop_length=HOP_LENGTH)
    
    # åˆä½µç‰¹å¾µä¸¦è½‰ç½® -> (N_frames, 13)
    features = np.vstack([chroma, rms])
    
    progress(1.0, desc="ç‰¹å¾µæå–å®Œæˆ")
    return features.T

# æ ¸å¿ƒé‚è¼¯ 2: DTW å°é½Š (DTW Alignment)
def align_dtw(features_input, features_ref):
    """
    åŸ·è¡Œå‹•æ…‹æ™‚é–“è¦æ•´ (DTW) ä»¥å°é½Šå…©æ®µéŸ³è¨Šçš„æ™‚é–“è»¸ã€‚
    """
    # ä½¿ç”¨æ­å¹¾é‡Œå¾—è·é›¢ä½œç‚ºæˆæœ¬åº¦é‡
    D, wp = librosa.sequence.dtw(
        X=features_input.T,
        Y=features_ref.T,
        metric='euclidean'
    )
    return D, wp

# æ ¸å¿ƒé‚è¼¯ 3: çµæœåˆ†æèˆ‡å»ºè­°ç”Ÿæˆ (Analysis and Feedback Generation)
def analyze_results(wp, D, features_input, features_ref):
    """æ ¹æ“š DTW çµæœåˆ†æé€Ÿåº¦å’ŒéŸ³é«˜å·®ç•°ï¼Œä¸¦ç”Ÿæˆå»ºè­°ã€‚"""
    feedback = []
    
    # --- 1. æ•´é«”ç›¸ä¼¼åº¦åˆ†æ•¸ (Overall Similarity Score) ---
    # æ­£è¦åŒ– DTW æˆæœ¬ (å€¼è¶Šä½è¶Šç›¸ä¼¼)
    normalized_cost = D[-1, -1] / len(wp)
    
    # å°‡æˆæœ¬è½‰æ›ç‚º 0-100 çš„åˆ†æ•¸ï¼Œè¶Šé«˜è¶Šå¥½
    k = 2.0 
    similarity_score = 100 * np.exp(-k * normalized_cost)
    
    # åµæ¸¬æ˜¯å¦ç‚ºä¸åŒæ­Œæ›²
    DIFFERENT_SONG_THRESHOLD = 1.0 
    if normalized_cost > DIFFERENT_SONG_THRESHOLD:
        feedback.append(
            "**åˆ†æè­¦ç¤ºï¼š** å…©æ®µéŸ³è¨Šçš„å·®ç•°éå¤§ï¼Œç³»çµ±åˆ¤æ–·å¯èƒ½ä¾†è‡ª**ä¸åŒçš„æ­Œæ›²**ã€‚"
            "å› æ­¤ç›¸ä¼¼åº¦åˆ†æ•¸æ¥µä½ï¼Œä»¥ä¸‹çš„å±€éƒ¨å»ºè­°å¯èƒ½ä¸å…·åƒè€ƒåƒ¹å€¼ã€‚"
        )
        return f"{similarity_score:.1f}", '\n'.join(feedback)

    # --- 2. æ•´é«”ç¯€å¥/é€Ÿåº¦åˆ†æ (Global Tempo Analysis) ---
    input_frames = features_input.shape[0]
    ref_frames = features_ref.shape[0]
    avg_slope = input_frames / ref_frames
    
    tempo_suggestion = "**æ•´é«”ç¯€å¥è©•ä¼°:** "
    if avg_slope > 1.15:
        tempo_suggestion += f"æ‚¨çš„æ¼”å”±é€Ÿåº¦æ¯”åƒè€ƒéŸ³è¨Šæ…¢äº†ç´„ {avg_slope * 100 - 100:.0f}%ï¼Œå»ºè­°æ•´é«”åŠ å¿«ã€‚"
    elif avg_slope < 0.85:
        tempo_suggestion += f"æ‚¨çš„æ¼”å”±é€Ÿåº¦æ¯”åƒè€ƒéŸ³è¨Šå¿«äº†ç´„ {100 - avg_slope * 100:.0f}%ï¼Œå»ºè­°æ•´é«”æ”¾æ…¢ã€‚"
    else:
        tempo_suggestion += "æ‚¨çš„æ•´é«”ç¯€å¥æŒæ¡å¾—å¾ˆå¥½ï¼Œèˆ‡åƒè€ƒéŸ³è¨Šå¤§è‡´åŒæ­¥ã€‚"
    feedback.append(tempo_suggestion)
    
    # --- 3. å±€éƒ¨æ™‚åºèˆ‡éŸ³é«˜å•é¡Œ (Local Timing and Pitch Issues) ---
    feedback.append("\n**å…·é«”å±€éƒ¨æ”¹é€²å»ºè­°:**")
    
    # è¨ˆç®—è·¯å¾‘ä¸Šæ¯å€‹é»çš„å±€éƒ¨æˆæœ¬
    local_costs = np.array([D[i, j] for i, j in wp])
    # è¨ˆç®—æ¯ä¸€æ­¥çš„æˆæœ¬å¢é‡ï¼Œæ›´èƒ½åæ‡‰å•é¡Œé»
    step_costs = np.diff(local_costs, prepend=0)

    # æ‰¾å‡ºæˆæœ¬å¢é‡é¡¯è‘—é«˜æ–¼å¹³å‡å€¼çš„é»
    mean_step_cost = np.mean(step_costs)
    std_step_cost = np.std(step_costs)
    threshold = mean_step_cost + 1.5 * std_step_cost
    
    high_cost_indices = np.where(step_costs > threshold)[0]
    
    if high_cost_indices.size == 0:
        feedback.append("â€¢ è¡¨ç¾å„ªç•°ï¼æœªæª¢æ¸¬åˆ°æ˜é¡¯çš„å±€éƒ¨æ™‚åºæˆ–éŸ³é«˜å•é¡Œã€‚")
    else:
        # å°‡é€£çºŒçš„é«˜æˆæœ¬é»åˆ†çµ„ç‚ºå•é¡Œå€æ®µ
        issue_groups = []
        if high_cost_indices.size > 0:
            current_group = [high_cost_indices[0]]
            for i in range(1, len(high_cost_indices)):
                # å¦‚æœç´¢å¼•æ˜¯é€£çºŒçš„ï¼Œå‰‡è¦–ç‚ºåŒä¸€å•é¡Œ
                if high_cost_indices[i] == high_cost_indices[i-1] + 1:
                    current_group.append(high_cost_indices[i])
                else:
                    issue_groups.append(current_group)
                    current_group = [high_cost_indices[i]]
            issue_groups.append(current_group)
        
        # åˆ†ææ¯å€‹å•é¡Œå€æ®µ
        TIME_PER_FRAME = HOP_LENGTH / TARGET_SR
        for group in issue_groups[:5]: # æœ€å¤šé¡¯ç¤ºå‰ 5 å€‹å•é¡Œ
            start_k, end_k = group[0], group[-1]
            i_start, j_start = wp[start_k]
            i_end, j_end = wp[end_k]
            
            time_start = i_start * TIME_PER_FRAME
            time_end = i_end * TIME_PER_FRAME
            
            # æå–è©²å€æ®µçš„ Chroma ç‰¹å¾µ
            chroma_input_seg = features_input[i_start:i_end+1, :12]
            chroma_ref_seg = features_ref[j_start:j_end+1, :12]
            
            suggestion = ""
            if chroma_input_seg.size > 0 and chroma_ref_seg.size > 0:
                # è¨ˆç®—å±€éƒ¨é€Ÿåº¦å·®ç•°
                frames_input_seg = i_end - i_start
                frames_ref_seg = j_end - j_start
                local_slope = frames_input_seg / (frames_ref_seg + 1e-6)

                # è¨ˆç®—éŸ³é«˜å·®ç•°
                input_peak_bin = np.mean(np.argmax(chroma_input_seg, axis=1))
                ref_peak_bin = np.mean(np.argmax(chroma_ref_seg, axis=1))
                pitch_diff = input_peak_bin - ref_peak_bin
                
                if abs(pitch_diff) > 0.8: # éŸ³é«˜å·®ç•°é¡¯è‘—
                    pitch_level = "åé«˜" if pitch_diff > 0 else "åä½"
                    suggestion = f"éŸ³é«˜æ˜é¡¯**{pitch_level}** (åå·®ç´„ {abs(pitch_diff):.1f} å€‹åŠéŸ³)ã€‚"
                elif local_slope > 1.8:
                    suggestion = "**ç¯€å¥æ‹–æ²“**ï¼Œæ¼”å”±é€Ÿåº¦éæ…¢ã€‚"
                elif local_slope < 0.5:
                    suggestion = "**ç¯€å¥æ¶æ‹**ï¼Œæ¼”å”±é€Ÿåº¦éå¿«ã€‚"
                else:
                    suggestion = "éŸ³æº–æˆ–éŸ³è‰²ä¸åŒ¹é…ï¼Œè«‹æ³¨æ„æ­¤è™•çš„ç™¼è²ç©©å®šæ€§ã€‚"
            
            if time_end - time_start < TIME_PER_FRAME:
                feedback.append(f"â€¢ **åœ¨ {time_start:.2f} ç§’é™„è¿‘:** {suggestion}")
            else:
                feedback.append(f"â€¢ **æ™‚é–“ {time_start:.2f} ç§’ - {time_end:.2f} ç§’:** {suggestion}")
        
        if len(issue_groups) > 5:
            feedback.append("â€¢ ... (åƒ…é¡¯ç¤ºå‰ 5 å€‹æœ€é¡¯è‘—çš„å•é¡Œé»)")

    return f"{similarity_score:.1f}", '\n'.join(feedback)

# æ ¸å¿ƒé‚è¼¯ 4: DTW å¯è¦–åŒ– (DTW Visualization)
def plot_dtw_path(D, wp):
    """ç¹ªè£½ç´¯ç©æˆæœ¬çŸ©é™£å’Œæœ€ä½³è¦æ•´è·¯å¾‘ã€‚"""
    fig = plt.figure(figsize=(8, 8))
    ax = fig.add_subplot(111)
    
    img = librosa.display.specshow(D, sr=TARGET_SR, x_axis='frames', y_axis='frames', ax=ax, hop_length=HOP_LENGTH)
    fig.colorbar(img, ax=ax, label='ç´¯ç©æˆæœ¬')
    ax.set(title='DTW ç´¯ç©æˆæœ¬èˆ‡æœ€ä½³è·¯å¾‘')
    
    # ç¹ªè£½æœ€ä½³è·¯å¾‘
    ax.plot(wp[:, 1], wp[:, 0], marker='.', color='red', linestyle='-', linewidth=2, alpha=0.6)
    
    ax.set_xlabel("åƒè€ƒéŸ³è¨Š (å½±æ ¼)")
    ax.set_ylabel("æ‚¨çš„æ­Œè² (å½±æ ¼)")
    plt.tight_layout()
    return fig

# æ··åˆéŸ³è¨Š (Mix Audio)
def mix_audio(path1, path2):
    """å°‡å…©æ®µéŸ³è¨Šæ··åˆæˆä¸€å€‹æª”æ¡ˆä»¥ä¾¿æ–¼æ¯”è¼ƒã€‚"""
    try:
        y1, _ = librosa.load(path1, sr=TARGET_SR, mono=True)
        y2, _ = librosa.load(path2, sr=TARGET_SR, mono=True)

        # å¡«å……è¼ƒçŸ­çš„éŸ³è¨Š
        len1, len2 = len(y1), len(y2)
        if len1 > len2:
            y2 = np.pad(y2, (0, len1 - len2))
        else:
            y1 = np.pad(y1, (0, len2 - len1))

        mixed = y1 + y2
        
        # æ­£è¦åŒ–ä»¥é˜²æ­¢å‰Šæ³¢
        max_amp = np.max(np.abs(mixed))
        if max_amp > 0:
            mixed /= max_amp
        
        # å„²å­˜åˆ°æš«å­˜æª”
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as fp:
            sf.write(fp.name, mixed, TARGET_SR)
            return fp.name
    except Exception as e:
        print(f"æ··åˆéŸ³è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def singing_evaluator(input_audio_path, ref_audio_path, generate_video=True, progress=gr.Progress()):
    """Gradio ä»‹é¢çš„ä¸»è¦è™•ç†å‡½æ•¸ - ä½¿ç”¨è¦–çª—åˆ†æã€‚"""
    if not input_audio_path or not ref_audio_path:
        raise gr.Error("è«‹åŒæ™‚ä¸Šå‚³æˆ–éŒ„è£½æ‚¨çš„æ­Œè²å’Œåƒè€ƒéŸ³è¨Šã€‚")

    try:
        # 1. ç‰¹å¾µæå–
        progress(0, desc="é–‹å§‹åˆ†æ...")
        progress(0.05, desc="æå–è¼¸å…¥éŸ³è¨Šç‰¹å¾µ...")
        features_input = extract_features(input_audio_path, progress)
        
        progress(0.15, desc="æå–åƒè€ƒéŸ³è¨Šç‰¹å¾µ...")
        features_ref = extract_features(ref_audio_path, progress)
        
        # 2. è¦–çª—å¼åˆ†æï¼ˆ5ç§’è¦–çª—ï¼Œ2ç§’é‡ç–Šï¼‰
        progress(0.25, desc="åŸ·è¡Œè¦–çª—å¼éŸ³æº–åˆ†æ...")
        window_results = windowed_pitch_analysis(features_input, features_ref, 
                                                 window_size=5.0, overlap=2.0, progress=progress)
        
        # 3. è¨ˆç®—æ•´é«”åˆ†æ•¸
        progress(0.55, desc="è¨ˆç®—æ•´é«”åˆ†æ•¸...")
        if window_results:
            avg_score = np.mean([r['overall_score'] for r in window_results])
            similarity_score = f"{avg_score:.1f}"
        else:
            similarity_score = "N/A"
        
        # 4. ç”Ÿæˆæ–‡å­—å»ºè­°
        progress(0.6, desc="ç”Ÿæˆæ”¹é€²å»ºè­°...")
        feedback_text = generate_windowed_feedback(window_results)
        
        # 5. å¯è¦–åŒ–è¦–çª—åˆ†æçµæœ
        progress(0.65, desc="ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...")
        windowed_plot = plot_windowed_analysis(window_results)

        # 6. æ··åˆéŸ³è¨Š
        progress(0.7, desc="æ··åˆéŸ³è¨Š...")
        mixed_audio_path = mix_audio(input_audio_path, ref_audio_path)
        
        # 7. é è¨ˆç®—å³æ™‚éŸ³é«˜æ•¸æ“š
        progress(0.75, desc="é è¨ˆç®—å³æ™‚éŸ³é«˜æ•¸æ“š...")
        pitch_timeline = precompute_pitch_differences(features_input, features_ref, interval=0.1, progress=progress)
        
        # 8. ç”Ÿæˆå‹•ç•«å½±ç‰‡ï¼ˆå¯é¸ï¼‰
        animation_video_path = None
        if generate_video and pitch_timeline:
            try:
                progress(0.85, desc="ç”Ÿæˆå‹•ç•«å½±ç‰‡...")
                print("ğŸ¬ é–‹å§‹ç”Ÿæˆå‹•ç•«å½±ç‰‡...")
                animation_video_path = generate_pitch_animation_video(
                    pitch_timeline, mixed_audio_path, progress=progress
                )
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå‹•ç•«å½±ç‰‡å¤±æ•—: {e}")
                animation_video_path = None
        
        # 9. è¿”å›æ‰€æœ‰çµæœ
        progress(1.0, desc="åˆ†æå®Œæˆï¼")
        return (similarity_score, feedback_text, windowed_plot, 
                input_audio_path, ref_audio_path, mixed_audio_path,
                animation_video_path)
        
    except gr.Error as e:
        raise e
    except Exception as e:
        error_message = f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}"
        print(error_message)
        raise gr.Error("åˆ†æå¤±æ•—ï¼Œè«‹æª¢æŸ¥æ‚¨çš„éŸ³è¨Šæª”æ¡ˆæ˜¯å¦æœ‰æ•ˆï¼Œæˆ–ç¨å¾Œå†è©¦ã€‚")
    
# --- Gradio ç•Œé¢å®šç¾© ---
title = "ğŸ™ï¸ AI æ­Œè²ç›¸ä¼¼æ€§è©•ä¼°èˆ‡è¼”åŠ©ç³»çµ± ğŸ¶"
description = (
    "ä¸Šå‚³æˆ–**å³æ™‚éŒ„è£½**æ‚¨çš„æ­Œè²å’Œåƒè€ƒéŸ³è¨Šï¼Œç³»çµ±å°‡ä½¿ç”¨**æ»‘å‹•è¦–çª—åˆ†æ**ï¼ˆæ¯5ç§’ä¸€å€‹è¦–çª—ï¼Œé‡ç–Š2ç§’ï¼‰ï¼Œ"
    "é€éå‹•æ…‹æ™‚é–“è¦æ•´ (DTW) æŠ€è¡“ï¼Œå°æ¯å€‹æ™‚æ®µçš„**éŸ³æº–ã€ç¯€å¥**é€²è¡Œç¨ç«‹è©•ä¼°ã€‚"
    "æä¾› **0-100 çš„æ•´é«”åˆ†æ•¸**ã€**æ™‚é–“åºåˆ—åˆ†æåœ–è¡¨**ï¼Œä»¥åŠ**å‹•æ…‹å½±ç‰‡å¼å³æ™‚éŸ³é«˜åˆ†æ**ã€‚"
)

with gr.Blocks(theme=gr.themes.Soft(), title=title) as demo:
    gr.Markdown(f"# {title}")
    gr.Markdown(description)

    with gr.Row():
        input_audio = gr.Audio(type="filepath", label="ğŸ¤ æ‚¨çš„æ­Œè² (Input)", sources=["upload", "microphone"])
        ref_audio = gr.Audio(type="filepath", label="ğŸ§ åƒè€ƒéŸ³è¨Š (Reference)", sources=["upload", "microphone"])
    
    with gr.Row():
        analyze_btn = gr.Button("ğŸš€ é–‹å§‹åˆ†æèˆ‡è©•ä¼°", variant="primary", scale=3)
        generate_video_checkbox = gr.Checkbox(label="ç”Ÿæˆå‹•ç•«å½±ç‰‡ï¼ˆéœ€è¦è¼ƒé•·æ™‚é–“ï¼‰", value=True, scale=1)
    
    result_outputs_group = gr.Column(visible=False) 
    with result_outputs_group:
        gr.Markdown("---")
        gr.Markdown("## ğŸ“‹ è©•ä¼°å ±å‘Š")
        
        with gr.Row():
            score_display = gr.Textbox(label="ç¸½é«”ç›¸ä¼¼åº¦åˆ†æ•¸ (0-100åˆ†ï¼Œè¶Šé«˜è¶Šå¥½)", scale=1)
        
        feedback_output = gr.Markdown(label="### ğŸ“œ å…·é«”æ”¹é€²å»ºè­°")

        windowed_plot_output = gr.Plot(label="ğŸ“Š è¦–çª—å¼åˆ†æåœ–è¡¨ (éŸ³æº–èˆ‡ç¯€å¥éš¨æ™‚é–“è®ŠåŒ–)")
        
        gr.Markdown("---")
        gr.Markdown("## ğŸ”Š éŸ³è¨Šæ¯”å°æ’­æ”¾")
        
        with gr.Row():
            input_playback = gr.Audio(label="æ‚¨çš„æ­Œè² (Input)")
            ref_playback = gr.Audio(label="åƒè€ƒéŸ³è¨Š (Reference)")
        
        mixed_playback = gr.Audio(label="ğŸ›ï¸ ç–ŠåŠ æ’­æ”¾ (Mixed for Comparison)")
        gr.Markdown("*æç¤ºï¼šæ’­æ”¾ã€Œç–ŠåŠ æ’­æ”¾ã€éŸ³è»Œï¼Œå¯ä»¥å¹«åŠ©æ‚¨æ›´æ¸…æ™°åœ°è½å‡ºç¯€å¥å’ŒéŸ³æº–çš„å·®ç•°ã€‚*")
        
        gr.Markdown("---")
        gr.Markdown("## ğŸ¬ å‹•æ…‹å³æ™‚éŸ³é«˜åˆ†æå½±ç‰‡")
        gr.Markdown("*å½±ç‰‡æœƒè‡ªå‹•èˆ‡éŸ³è¨ŠåŒæ­¥æ’­æ”¾ï¼Œå±•ç¤ºæ¯å€‹æ™‚åˆ»çš„éŸ³é«˜è®ŠåŒ–å’Œåå·®åˆ†æ*")
        
        animation_video_output = gr.Video(label="ğŸµ å³æ™‚éŸ³é«˜åˆ†æå‹•ç•«", autoplay=False)
        gr.Markdown("ğŸ’¡ **ä½¿ç”¨æç¤º**: é»æ“Šæ’­æ”¾æŒ‰éˆ•ï¼Œå½±ç‰‡æœƒåŒæ­¥é¡¯ç¤ºéŸ³é«˜åˆ†æå‹•ç•«ï¼Œè®“æ‚¨æ¸…æ¥šçœ‹åˆ°æ¯å€‹æ™‚é–“é»çš„è¡¨ç¾")

    # ä¸»åˆ†ææµç¨‹
    def run_analysis(input_audio_path, ref_audio_path, should_generate_video):
        (score, feedback, plot, _, _, mixed_path, 
         animation_path) = singing_evaluator(input_audio_path, ref_audio_path, 
                                            generate_video=should_generate_video)
        
        return {
            result_outputs_group: gr.Column(visible=True),
            score_display: score,
            feedback_output: feedback,
            windowed_plot_output: plot,
            input_playback: gr.Audio(value=input_audio_path, label="æ‚¨çš„æ­Œè² (Input)"),
            ref_playback: gr.Audio(value=ref_audio_path, label="åƒè€ƒéŸ³è¨Š (Reference)"),
            mixed_playback: gr.Audio(value=mixed_path, label="ğŸ›ï¸ ç–ŠåŠ æ’­æ”¾ (Mixed for Comparison)"),
            animation_video_output: gr.Video(value=animation_path, label="ğŸµ å³æ™‚éŸ³é«˜åˆ†æå‹•ç•«")
        }

    # äº‹ä»¶ç¶å®š
    analyze_btn.click(
        fn=lambda: gr.Column(visible=False),
        outputs=[result_outputs_group]
    ).then(
        fn=run_analysis,
        inputs=[input_audio, ref_audio, generate_video_checkbox],
        outputs=[result_outputs_group, score_display, feedback_output, windowed_plot_output, 
                input_playback, ref_playback, mixed_playback, animation_video_output]
    )

if __name__ == "__main__":
    demo.launch(share=True)